{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ssNjisJBh7oa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f4-xFuMkmYu6"
      },
      "outputs": [],
      "source": [
        "num_classes = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gP7QODCvkAGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bba8d7b-3ce5-4caa-9969-fa8d45245830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access files if necessary\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WWfi49aVkA0j"
      },
      "outputs": [],
      "source": [
        "# Path to your .h5 file in Google Drive\n",
        "file_path1 = '/content/drive/My Drive/dataset/LSTM input/NTU_CS_part_aware_lstm.h5'\n",
        "file_path2 = '/content/drive/My Drive/dataset/LSTM input/NTU_CV_part_aware_lstm.h5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrOZryGslQOM",
        "outputId": "b02ff7cc-7060-4c84-f962-d887fd8c0973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in CS file: ['test_x', 'test_y', 'valid_x', 'valid_y', 'x', 'y']\n",
            "Keys in CV file: ['test_x', 'test_y', 'valid_x', 'valid_y', 'x', 'y']\n"
          ]
        }
      ],
      "source": [
        "# Load the NTU CS and CV datasets from .h5 files\n",
        "with h5py.File(file_path1, 'r') as cs_file, h5py.File(file_path2, 'r') as cv_file:\n",
        "    # List all keys in the files\n",
        "    print(\"Keys in CS file:\", list(cs_file.keys()))\n",
        "    print(\"Keys in CV file:\", list(cv_file.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "MwPR3LbnlkUO",
        "outputId": "a939667f-f08b-433e-cddc-4abd9c3f1ff4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# Load the NTU CS and CV datasets from .h5 files\\nwith h5py.File(file_path1, 'r') as cs_file, h5py.File(file_path2, 'r') as cv_file:\\n    # Access the datasets using the keys\\n    cs_data = np.array(cs_file['x'])\\n    cs_labels = np.array(cs_file['y'])\\n    cv_data = np.array(cv_file['x'])\\n    cv_labels = np.array(cv_file['y'])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"# Load the NTU CS and CV datasets from .h5 files\n",
        "with h5py.File(file_path1, 'r') as cs_file, h5py.File(file_path2, 'r') as cv_file:\n",
        "    # Access the datasets using the keys\n",
        "    cs_data = np.array(cs_file['x'])\n",
        "    cs_labels = np.array(cs_file['y'])\n",
        "    cv_data = np.array(cv_file['x'])\n",
        "    cv_labels = np.array(cv_file['y'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XD7JPSS5xpH4"
      },
      "outputs": [],
      "source": [
        "# Load the NTU CS and CV datasets from .h5 files\n",
        "with h5py.File(file_path1, 'r') as cs_file, h5py.File(file_path2, 'r') as cv_file:\n",
        "    # Access the datasets using the keys\n",
        "    x_train_CS = np.array(cs_file['x'])\n",
        "    y_train_CS = np.array(cs_file['y'])\n",
        "    x_valid_CS = np.array(cs_file['valid_x'])\n",
        "    y_valid_CS = np.array(cs_file['valid_y'])\n",
        "    x_test_CS = np.array(cs_file['test_x'])\n",
        "    y_test_CS = np.array(cs_file['test_y'])\n",
        "\n",
        "    x_train_CV = np.array(cv_file['x'])\n",
        "    y_train_CV = np.array(cv_file['y'])\n",
        "    x_valid_CV = np.array(cv_file['valid_x'])\n",
        "    y_valid_CV = np.array(cv_file['valid_y'])\n",
        "    x_test_CV = np.array(cv_file['test_x'])\n",
        "    y_test_CV = np.array(cv_file['test_y'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Add, Activation, BatchNormalization, AveragePooling2D, Reshape\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Multiply, Add\n",
        "\n",
        "def residual_block(input_tensor, filters, kernel_size, stage, block, strides=(1, 1)):\n",
        "    conv_name_base = f\"res{stage}_{block}_branch\"\n",
        "    bn_name_base = f\"bn{stage}_{block}_branch\"\n",
        "\n",
        "    x = Conv2D(filters, kernel_size, strides=strides, padding='same', name=conv_name_base + '2a')(input_tensor)\n",
        "    x = BatchNormalization(axis=3, name=bn_name_base + '2a')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
        "    x = BatchNormalization(axis=3, name=bn_name_base + '2b')(x)\n",
        "\n",
        "    shortcut = Conv2D(filters, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(shortcut)\n",
        "\n",
        "    x = Add()([x, shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def channel_attention(input_tensor, ratio):\n",
        "    filters = input_tensor.shape[-1]\n",
        "\n",
        "    avg_pool = GlobalAveragePooling2D()(input_tensor)\n",
        "    avg_pool = Dense(filters // ratio, activation='relu')(avg_pool)\n",
        "    avg_pool = Dense(filters, activation='sigmoid')(avg_pool)\n",
        "    avg_pool = Reshape((1, 1, filters))(avg_pool)\n",
        "\n",
        "    max_pool = GlobalMaxPooling2D()(input_tensor)\n",
        "    max_pool = Dense(filters // ratio, activation='relu')(max_pool)\n",
        "    max_pool = Dense(filters, activation='sigmoid')(max_pool)\n",
        "    max_pool = Reshape((1, 1, filters))(max_pool)\n",
        "\n",
        "    output_tensor = Add()([avg_pool, max_pool])\n",
        "    output_tensor = Activation('sigmoid')(output_tensor)\n",
        "    return Multiply()([input_tensor, output_tensor])\n",
        "\n",
        "def EfficientNet_with_attention(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Implement EfficientNet architecture here\n",
        "\n",
        "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(inputs)\n",
        "    x = BatchNormalization(axis=3, name='bn_conv1')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = residual_block(x, 64, (3, 3), stage=2, block='a', strides=(1, 1))\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 64, (3, 3), stage=2, block='b')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 64, (3, 3), stage=2, block='c')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "\n",
        "    x = residual_block(x, 128, (3, 3), stage=3, block='a', strides=(2, 2))\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 128, (3, 3), stage=3, block='b')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 128, (3, 3), stage=3, block='c')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 128, (3, 3), stage=3, block='d')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "\n",
        "    x = residual_block(x, 256, (3, 3), stage=4, block='a', strides=(2, 2))\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 256, (3, 3), stage=4, block='b')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 256, (3, 3), stage=4, block='c')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 256, (3, 3), stage=4, block='d')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 256, (3, 3), stage=4, block='e')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 256, (3, 3), stage=4, block='f')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "\n",
        "    x = residual_block(x, 512, (3, 3), stage=5, block='a', strides=(2, 2))\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 512, (3, 3), stage=5, block='b')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "    x = residual_block(x, 512, (3, 3), stage=5, block='c')\n",
        "    x = channel_attention(x, 16)  # Applying channel attention\n",
        "\n",
        "    x = MaxPooling2D((2, 2), name='max_pool')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(num_classes, activation='softmax', name='fc' + str(num_classes))(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name='efficientnet_with_attention')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming your input parameters\n",
        "num_classes = 60\n",
        "segment_length = 237\n",
        "num_features = 150\n",
        "\n",
        "# Create the modified EfficientNet model with channel attention\n",
        "input_shape = (segment_length, num_features, 1)  # Assuming grayscale images\n",
        "efficientnet_model_with_attention = EfficientNet_with_attention(input_shape, num_classes)\n",
        "\n",
        "# Print model architecture\n",
        "print(efficientnet_model_with_attention.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoJ7JGwBVkSY",
        "outputId": "3341d45b-7fc5-4aa9-a6fd-8815ede31e02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"efficientnet_with_attention\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 237, 150, 1)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1 (Conv2D)              (None, 119, 75, 64)          3200      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " bn_conv1 (BatchNormalizati  (None, 119, 75, 64)          256       ['conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 119, 75, 64)          0         ['bn_conv1[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 59, 37, 64)           0         ['activation[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " res2_a_branch2a (Conv2D)    (None, 59, 37, 64)           36928     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " bn2_a_branch2a (BatchNorma  (None, 59, 37, 64)           256       ['res2_a_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 59, 37, 64)           0         ['bn2_a_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res2_a_branch2b (Conv2D)    (None, 59, 37, 64)           36928     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " res2_a_branch1 (Conv2D)     (None, 59, 37, 64)           4160      ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " bn2_a_branch2b (BatchNorma  (None, 59, 37, 64)           256       ['res2_a_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn2_a_branch1 (BatchNormal  (None, 59, 37, 64)           256       ['res2_a_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 59, 37, 64)           0         ['bn2_a_branch2b[0][0]',      \n",
            "                                                                     'bn2_a_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 59, 37, 64)           0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 64)                   0         ['activation_2[0][0]']        \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            " global_max_pooling2d (Glob  (None, 64)                   0         ['activation_2[0][0]']        \n",
            " alMaxPooling2D)                                                                                  \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 4)                    260       ['global_average_pooling2d[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 4)                    260       ['global_max_pooling2d[0][0]']\n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 64)                   320       ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 64)                   320       ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " reshape (Reshape)           (None, 1, 1, 64)             0         ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)         (None, 1, 1, 64)             0         ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 1, 1, 64)             0         ['reshape[0][0]',             \n",
            "                                                                     'reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 1, 1, 64)             0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 59, 37, 64)           0         ['activation_2[0][0]',        \n",
            "                                                                     'activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " res2_b_branch2a (Conv2D)    (None, 59, 37, 64)           36928     ['multiply[0][0]']            \n",
            "                                                                                                  \n",
            " bn2_b_branch2a (BatchNorma  (None, 59, 37, 64)           256       ['res2_b_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 59, 37, 64)           0         ['bn2_b_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res2_b_branch2b (Conv2D)    (None, 59, 37, 64)           36928     ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " res2_b_branch1 (Conv2D)     (None, 59, 37, 64)           4160      ['multiply[0][0]']            \n",
            "                                                                                                  \n",
            " bn2_b_branch2b (BatchNorma  (None, 59, 37, 64)           256       ['res2_b_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn2_b_branch1 (BatchNormal  (None, 59, 37, 64)           256       ['res2_b_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 59, 37, 64)           0         ['bn2_b_branch2b[0][0]',      \n",
            "                                                                     'bn2_b_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 59, 37, 64)           0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 64)                   0         ['activation_5[0][0]']        \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_1 (Gl  (None, 64)                   0         ['activation_5[0][0]']        \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 4)                    260       ['global_average_pooling2d_1[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 4)                    260       ['global_max_pooling2d_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 64)                   320       ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 64)                   320       ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)         (None, 1, 1, 64)             0         ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_3 (Reshape)         (None, 1, 1, 64)             0         ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 1, 1, 64)             0         ['reshape_2[0][0]',           \n",
            "                                                                     'reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 1, 1, 64)             0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)       (None, 59, 37, 64)           0         ['activation_5[0][0]',        \n",
            "                                                                     'activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " res2_c_branch2a (Conv2D)    (None, 59, 37, 64)           36928     ['multiply_1[0][0]']          \n",
            "                                                                                                  \n",
            " bn2_c_branch2a (BatchNorma  (None, 59, 37, 64)           256       ['res2_c_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 59, 37, 64)           0         ['bn2_c_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res2_c_branch2b (Conv2D)    (None, 59, 37, 64)           36928     ['activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " res2_c_branch1 (Conv2D)     (None, 59, 37, 64)           4160      ['multiply_1[0][0]']          \n",
            "                                                                                                  \n",
            " bn2_c_branch2b (BatchNorma  (None, 59, 37, 64)           256       ['res2_c_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn2_c_branch1 (BatchNormal  (None, 59, 37, 64)           256       ['res2_c_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 59, 37, 64)           0         ['bn2_c_branch2b[0][0]',      \n",
            "                                                                     'bn2_c_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 59, 37, 64)           0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2  (None, 64)                   0         ['activation_8[0][0]']        \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_2 (Gl  (None, 64)                   0         ['activation_8[0][0]']        \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 4)                    260       ['global_average_pooling2d_2[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_10 (Dense)            (None, 4)                    260       ['global_max_pooling2d_2[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 64)                   320       ['dense_8[0][0]']             \n",
            "                                                                                                  \n",
            " dense_11 (Dense)            (None, 64)                   320       ['dense_10[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_4 (Reshape)         (None, 1, 1, 64)             0         ['dense_9[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)         (None, 1, 1, 64)             0         ['dense_11[0][0]']            \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 1, 1, 64)             0         ['reshape_4[0][0]',           \n",
            "                                                                     'reshape_5[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 1, 1, 64)             0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)       (None, 59, 37, 64)           0         ['activation_8[0][0]',        \n",
            "                                                                     'activation_9[0][0]']        \n",
            "                                                                                                  \n",
            " res3_a_branch2a (Conv2D)    (None, 30, 19, 128)          73856     ['multiply_2[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_a_branch2a (BatchNorma  (None, 30, 19, 128)          512       ['res3_a_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 30, 19, 128)          0         ['bn3_a_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res3_a_branch2b (Conv2D)    (None, 30, 19, 128)          147584    ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " res3_a_branch1 (Conv2D)     (None, 30, 19, 128)          8320      ['multiply_2[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_a_branch2b (BatchNorma  (None, 30, 19, 128)          512       ['res3_a_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn3_a_branch1 (BatchNormal  (None, 30, 19, 128)          512       ['res3_a_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 30, 19, 128)          0         ['bn3_a_branch2b[0][0]',      \n",
            "                                                                     'bn3_a_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 30, 19, 128)          0         ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3  (None, 128)                  0         ['activation_11[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_3 (Gl  (None, 128)                  0         ['activation_11[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 8)                    1032      ['global_average_pooling2d_3[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, 8)                    1032      ['global_max_pooling2d_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 128)                  1152      ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 128)                  1152      ['dense_14[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)         (None, 1, 1, 128)            0         ['dense_13[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_7 (Reshape)         (None, 1, 1, 128)            0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 1, 1, 128)            0         ['reshape_6[0][0]',           \n",
            "                                                                     'reshape_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 1, 1, 128)            0         ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)       (None, 30, 19, 128)          0         ['activation_11[0][0]',       \n",
            "                                                                     'activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " res3_b_branch2a (Conv2D)    (None, 30, 19, 128)          147584    ['multiply_3[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_b_branch2a (BatchNorma  (None, 30, 19, 128)          512       ['res3_b_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 30, 19, 128)          0         ['bn3_b_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res3_b_branch2b (Conv2D)    (None, 30, 19, 128)          147584    ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " res3_b_branch1 (Conv2D)     (None, 30, 19, 128)          16512     ['multiply_3[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_b_branch2b (BatchNorma  (None, 30, 19, 128)          512       ['res3_b_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn3_b_branch1 (BatchNormal  (None, 30, 19, 128)          512       ['res3_b_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 30, 19, 128)          0         ['bn3_b_branch2b[0][0]',      \n",
            "                                                                     'bn3_b_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 30, 19, 128)          0         ['add_8[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling2d_4  (None, 128)                  0         ['activation_14[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_4 (Gl  (None, 128)                  0         ['activation_14[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 8)                    1032      ['global_average_pooling2d_4[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_18 (Dense)            (None, 8)                    1032      ['global_max_pooling2d_4[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_17 (Dense)            (None, 128)                  1152      ['dense_16[0][0]']            \n",
            "                                                                                                  \n",
            " dense_19 (Dense)            (None, 128)                  1152      ['dense_18[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_8 (Reshape)         (None, 1, 1, 128)            0         ['dense_17[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_9 (Reshape)         (None, 1, 1, 128)            0         ['dense_19[0][0]']            \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 1, 1, 128)            0         ['reshape_8[0][0]',           \n",
            "                                                                     'reshape_9[0][0]']           \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 1, 1, 128)            0         ['add_9[0][0]']               \n",
            "                                                                                                  \n",
            " multiply_4 (Multiply)       (None, 30, 19, 128)          0         ['activation_14[0][0]',       \n",
            "                                                                     'activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " res3_c_branch2a (Conv2D)    (None, 30, 19, 128)          147584    ['multiply_4[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_c_branch2a (BatchNorma  (None, 30, 19, 128)          512       ['res3_c_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 30, 19, 128)          0         ['bn3_c_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res3_c_branch2b (Conv2D)    (None, 30, 19, 128)          147584    ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " res3_c_branch1 (Conv2D)     (None, 30, 19, 128)          16512     ['multiply_4[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_c_branch2b (BatchNorma  (None, 30, 19, 128)          512       ['res3_c_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn3_c_branch1 (BatchNormal  (None, 30, 19, 128)          512       ['res3_c_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 30, 19, 128)          0         ['bn3_c_branch2b[0][0]',      \n",
            "                                                                     'bn3_c_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 30, 19, 128)          0         ['add_10[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_5  (None, 128)                  0         ['activation_17[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_5 (Gl  (None, 128)                  0         ['activation_17[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 8)                    1032      ['global_average_pooling2d_5[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 8)                    1032      ['global_max_pooling2d_5[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 128)                  1152      ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " dense_23 (Dense)            (None, 128)                  1152      ['dense_22[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_10 (Reshape)        (None, 1, 1, 128)            0         ['dense_21[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_11 (Reshape)        (None, 1, 1, 128)            0         ['dense_23[0][0]']            \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 1, 1, 128)            0         ['reshape_10[0][0]',          \n",
            "                                                                     'reshape_11[0][0]']          \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 1, 1, 128)            0         ['add_11[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_5 (Multiply)       (None, 30, 19, 128)          0         ['activation_17[0][0]',       \n",
            "                                                                     'activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " res3_d_branch2a (Conv2D)    (None, 30, 19, 128)          147584    ['multiply_5[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_d_branch2a (BatchNorma  (None, 30, 19, 128)          512       ['res3_d_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 30, 19, 128)          0         ['bn3_d_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res3_d_branch2b (Conv2D)    (None, 30, 19, 128)          147584    ['activation_19[0][0]']       \n",
            "                                                                                                  \n",
            " res3_d_branch1 (Conv2D)     (None, 30, 19, 128)          16512     ['multiply_5[0][0]']          \n",
            "                                                                                                  \n",
            " bn3_d_branch2b (BatchNorma  (None, 30, 19, 128)          512       ['res3_d_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn3_d_branch1 (BatchNormal  (None, 30, 19, 128)          512       ['res3_d_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 30, 19, 128)          0         ['bn3_d_branch2b[0][0]',      \n",
            "                                                                     'bn3_d_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 30, 19, 128)          0         ['add_12[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_6  (None, 128)                  0         ['activation_20[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_6 (Gl  (None, 128)                  0         ['activation_20[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_24 (Dense)            (None, 8)                    1032      ['global_average_pooling2d_6[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_26 (Dense)            (None, 8)                    1032      ['global_max_pooling2d_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_25 (Dense)            (None, 128)                  1152      ['dense_24[0][0]']            \n",
            "                                                                                                  \n",
            " dense_27 (Dense)            (None, 128)                  1152      ['dense_26[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_12 (Reshape)        (None, 1, 1, 128)            0         ['dense_25[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_13 (Reshape)        (None, 1, 1, 128)            0         ['dense_27[0][0]']            \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 1, 1, 128)            0         ['reshape_12[0][0]',          \n",
            "                                                                     'reshape_13[0][0]']          \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 1, 1, 128)            0         ['add_13[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)       (None, 30, 19, 128)          0         ['activation_20[0][0]',       \n",
            "                                                                     'activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " res4_a_branch2a (Conv2D)    (None, 15, 10, 256)          295168    ['multiply_6[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_a_branch2a (BatchNorma  (None, 15, 10, 256)          1024      ['res4_a_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 15, 10, 256)          0         ['bn4_a_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res4_a_branch2b (Conv2D)    (None, 15, 10, 256)          590080    ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " res4_a_branch1 (Conv2D)     (None, 15, 10, 256)          33024     ['multiply_6[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_a_branch2b (BatchNorma  (None, 15, 10, 256)          1024      ['res4_a_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn4_a_branch1 (BatchNormal  (None, 15, 10, 256)          1024      ['res4_a_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 15, 10, 256)          0         ['bn4_a_branch2b[0][0]',      \n",
            "                                                                     'bn4_a_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 15, 10, 256)          0         ['add_14[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7  (None, 256)                  0         ['activation_23[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_7 (Gl  (None, 256)                  0         ['activation_23[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_28 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_7[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_30 (Dense)            (None, 16)                   4112      ['global_max_pooling2d_7[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_29 (Dense)            (None, 256)                  4352      ['dense_28[0][0]']            \n",
            "                                                                                                  \n",
            " dense_31 (Dense)            (None, 256)                  4352      ['dense_30[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_14 (Reshape)        (None, 1, 1, 256)            0         ['dense_29[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_15 (Reshape)        (None, 1, 1, 256)            0         ['dense_31[0][0]']            \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 1, 1, 256)            0         ['reshape_14[0][0]',          \n",
            "                                                                     'reshape_15[0][0]']          \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 1, 1, 256)            0         ['add_15[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)       (None, 15, 10, 256)          0         ['activation_23[0][0]',       \n",
            "                                                                     'activation_24[0][0]']       \n",
            "                                                                                                  \n",
            " res4_b_branch2a (Conv2D)    (None, 15, 10, 256)          590080    ['multiply_7[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_b_branch2a (BatchNorma  (None, 15, 10, 256)          1024      ['res4_b_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_25 (Activation)  (None, 15, 10, 256)          0         ['bn4_b_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res4_b_branch2b (Conv2D)    (None, 15, 10, 256)          590080    ['activation_25[0][0]']       \n",
            "                                                                                                  \n",
            " res4_b_branch1 (Conv2D)     (None, 15, 10, 256)          65792     ['multiply_7[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_b_branch2b (BatchNorma  (None, 15, 10, 256)          1024      ['res4_b_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn4_b_branch1 (BatchNormal  (None, 15, 10, 256)          1024      ['res4_b_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 15, 10, 256)          0         ['bn4_b_branch2b[0][0]',      \n",
            "                                                                     'bn4_b_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_26 (Activation)  (None, 15, 10, 256)          0         ['add_16[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8  (None, 256)                  0         ['activation_26[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_8 (Gl  (None, 256)                  0         ['activation_26[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_32 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_8[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_34 (Dense)            (None, 16)                   4112      ['global_max_pooling2d_8[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_33 (Dense)            (None, 256)                  4352      ['dense_32[0][0]']            \n",
            "                                                                                                  \n",
            " dense_35 (Dense)            (None, 256)                  4352      ['dense_34[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_16 (Reshape)        (None, 1, 1, 256)            0         ['dense_33[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_17 (Reshape)        (None, 1, 1, 256)            0         ['dense_35[0][0]']            \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 1, 1, 256)            0         ['reshape_16[0][0]',          \n",
            "                                                                     'reshape_17[0][0]']          \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 1, 1, 256)            0         ['add_17[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_8 (Multiply)       (None, 15, 10, 256)          0         ['activation_26[0][0]',       \n",
            "                                                                     'activation_27[0][0]']       \n",
            "                                                                                                  \n",
            " res4_c_branch2a (Conv2D)    (None, 15, 10, 256)          590080    ['multiply_8[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_c_branch2a (BatchNorma  (None, 15, 10, 256)          1024      ['res4_c_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 15, 10, 256)          0         ['bn4_c_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res4_c_branch2b (Conv2D)    (None, 15, 10, 256)          590080    ['activation_28[0][0]']       \n",
            "                                                                                                  \n",
            " res4_c_branch1 (Conv2D)     (None, 15, 10, 256)          65792     ['multiply_8[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_c_branch2b (BatchNorma  (None, 15, 10, 256)          1024      ['res4_c_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn4_c_branch1 (BatchNormal  (None, 15, 10, 256)          1024      ['res4_c_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_18 (Add)                (None, 15, 10, 256)          0         ['bn4_c_branch2b[0][0]',      \n",
            "                                                                     'bn4_c_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 15, 10, 256)          0         ['add_18[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_9  (None, 256)                  0         ['activation_29[0][0]']       \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " global_max_pooling2d_9 (Gl  (None, 256)                  0         ['activation_29[0][0]']       \n",
            " obalMaxPooling2D)                                                                                \n",
            "                                                                                                  \n",
            " dense_36 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_9[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " dense_38 (Dense)            (None, 16)                   4112      ['global_max_pooling2d_9[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_37 (Dense)            (None, 256)                  4352      ['dense_36[0][0]']            \n",
            "                                                                                                  \n",
            " dense_39 (Dense)            (None, 256)                  4352      ['dense_38[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_18 (Reshape)        (None, 1, 1, 256)            0         ['dense_37[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_19 (Reshape)        (None, 1, 1, 256)            0         ['dense_39[0][0]']            \n",
            "                                                                                                  \n",
            " add_19 (Add)                (None, 1, 1, 256)            0         ['reshape_18[0][0]',          \n",
            "                                                                     'reshape_19[0][0]']          \n",
            "                                                                                                  \n",
            " activation_30 (Activation)  (None, 1, 1, 256)            0         ['add_19[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)       (None, 15, 10, 256)          0         ['activation_29[0][0]',       \n",
            "                                                                     'activation_30[0][0]']       \n",
            "                                                                                                  \n",
            " res4_d_branch2a (Conv2D)    (None, 15, 10, 256)          590080    ['multiply_9[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_d_branch2a (BatchNorma  (None, 15, 10, 256)          1024      ['res4_d_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_31 (Activation)  (None, 15, 10, 256)          0         ['bn4_d_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res4_d_branch2b (Conv2D)    (None, 15, 10, 256)          590080    ['activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " res4_d_branch1 (Conv2D)     (None, 15, 10, 256)          65792     ['multiply_9[0][0]']          \n",
            "                                                                                                  \n",
            " bn4_d_branch2b (BatchNorma  (None, 15, 10, 256)          1024      ['res4_d_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn4_d_branch1 (BatchNormal  (None, 15, 10, 256)          1024      ['res4_d_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_20 (Add)                (None, 15, 10, 256)          0         ['bn4_d_branch2b[0][0]',      \n",
            "                                                                     'bn4_d_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_32 (Activation)  (None, 15, 10, 256)          0         ['add_20[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 256)                  0         ['activation_32[0][0]']       \n",
            " 0 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " global_max_pooling2d_10 (G  (None, 256)                  0         ['activation_32[0][0]']       \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_40 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_10[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_42 (Dense)            (None, 16)                   4112      ['global_max_pooling2d_10[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_41 (Dense)            (None, 256)                  4352      ['dense_40[0][0]']            \n",
            "                                                                                                  \n",
            " dense_43 (Dense)            (None, 256)                  4352      ['dense_42[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_20 (Reshape)        (None, 1, 1, 256)            0         ['dense_41[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_21 (Reshape)        (None, 1, 1, 256)            0         ['dense_43[0][0]']            \n",
            "                                                                                                  \n",
            " add_21 (Add)                (None, 1, 1, 256)            0         ['reshape_20[0][0]',          \n",
            "                                                                     'reshape_21[0][0]']          \n",
            "                                                                                                  \n",
            " activation_33 (Activation)  (None, 1, 1, 256)            0         ['add_21[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)      (None, 15, 10, 256)          0         ['activation_32[0][0]',       \n",
            "                                                                     'activation_33[0][0]']       \n",
            "                                                                                                  \n",
            " res4_e_branch2a (Conv2D)    (None, 15, 10, 256)          590080    ['multiply_10[0][0]']         \n",
            "                                                                                                  \n",
            " bn4_e_branch2a (BatchNorma  (None, 15, 10, 256)          1024      ['res4_e_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_34 (Activation)  (None, 15, 10, 256)          0         ['bn4_e_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res4_e_branch2b (Conv2D)    (None, 15, 10, 256)          590080    ['activation_34[0][0]']       \n",
            "                                                                                                  \n",
            " res4_e_branch1 (Conv2D)     (None, 15, 10, 256)          65792     ['multiply_10[0][0]']         \n",
            "                                                                                                  \n",
            " bn4_e_branch2b (BatchNorma  (None, 15, 10, 256)          1024      ['res4_e_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn4_e_branch1 (BatchNormal  (None, 15, 10, 256)          1024      ['res4_e_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_22 (Add)                (None, 15, 10, 256)          0         ['bn4_e_branch2b[0][0]',      \n",
            "                                                                     'bn4_e_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_35 (Activation)  (None, 15, 10, 256)          0         ['add_22[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 256)                  0         ['activation_35[0][0]']       \n",
            " 1 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " global_max_pooling2d_11 (G  (None, 256)                  0         ['activation_35[0][0]']       \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_44 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_11[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_46 (Dense)            (None, 16)                   4112      ['global_max_pooling2d_11[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_45 (Dense)            (None, 256)                  4352      ['dense_44[0][0]']            \n",
            "                                                                                                  \n",
            " dense_47 (Dense)            (None, 256)                  4352      ['dense_46[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_22 (Reshape)        (None, 1, 1, 256)            0         ['dense_45[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_23 (Reshape)        (None, 1, 1, 256)            0         ['dense_47[0][0]']            \n",
            "                                                                                                  \n",
            " add_23 (Add)                (None, 1, 1, 256)            0         ['reshape_22[0][0]',          \n",
            "                                                                     'reshape_23[0][0]']          \n",
            "                                                                                                  \n",
            " activation_36 (Activation)  (None, 1, 1, 256)            0         ['add_23[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)      (None, 15, 10, 256)          0         ['activation_35[0][0]',       \n",
            "                                                                     'activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " res4_f_branch2a (Conv2D)    (None, 15, 10, 256)          590080    ['multiply_11[0][0]']         \n",
            "                                                                                                  \n",
            " bn4_f_branch2a (BatchNorma  (None, 15, 10, 256)          1024      ['res4_f_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_37 (Activation)  (None, 15, 10, 256)          0         ['bn4_f_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res4_f_branch2b (Conv2D)    (None, 15, 10, 256)          590080    ['activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " res4_f_branch1 (Conv2D)     (None, 15, 10, 256)          65792     ['multiply_11[0][0]']         \n",
            "                                                                                                  \n",
            " bn4_f_branch2b (BatchNorma  (None, 15, 10, 256)          1024      ['res4_f_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn4_f_branch1 (BatchNormal  (None, 15, 10, 256)          1024      ['res4_f_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_24 (Add)                (None, 15, 10, 256)          0         ['bn4_f_branch2b[0][0]',      \n",
            "                                                                     'bn4_f_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_38 (Activation)  (None, 15, 10, 256)          0         ['add_24[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 256)                  0         ['activation_38[0][0]']       \n",
            " 2 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " global_max_pooling2d_12 (G  (None, 256)                  0         ['activation_38[0][0]']       \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_48 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_12[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_50 (Dense)            (None, 16)                   4112      ['global_max_pooling2d_12[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_49 (Dense)            (None, 256)                  4352      ['dense_48[0][0]']            \n",
            "                                                                                                  \n",
            " dense_51 (Dense)            (None, 256)                  4352      ['dense_50[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_24 (Reshape)        (None, 1, 1, 256)            0         ['dense_49[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_25 (Reshape)        (None, 1, 1, 256)            0         ['dense_51[0][0]']            \n",
            "                                                                                                  \n",
            " add_25 (Add)                (None, 1, 1, 256)            0         ['reshape_24[0][0]',          \n",
            "                                                                     'reshape_25[0][0]']          \n",
            "                                                                                                  \n",
            " activation_39 (Activation)  (None, 1, 1, 256)            0         ['add_25[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)      (None, 15, 10, 256)          0         ['activation_38[0][0]',       \n",
            "                                                                     'activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " res5_a_branch2a (Conv2D)    (None, 8, 5, 512)            1180160   ['multiply_12[0][0]']         \n",
            "                                                                                                  \n",
            " bn5_a_branch2a (BatchNorma  (None, 8, 5, 512)            2048      ['res5_a_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_40 (Activation)  (None, 8, 5, 512)            0         ['bn5_a_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res5_a_branch2b (Conv2D)    (None, 8, 5, 512)            2359808   ['activation_40[0][0]']       \n",
            "                                                                                                  \n",
            " res5_a_branch1 (Conv2D)     (None, 8, 5, 512)            131584    ['multiply_12[0][0]']         \n",
            "                                                                                                  \n",
            " bn5_a_branch2b (BatchNorma  (None, 8, 5, 512)            2048      ['res5_a_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn5_a_branch1 (BatchNormal  (None, 8, 5, 512)            2048      ['res5_a_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_26 (Add)                (None, 8, 5, 512)            0         ['bn5_a_branch2b[0][0]',      \n",
            "                                                                     'bn5_a_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_41 (Activation)  (None, 8, 5, 512)            0         ['add_26[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 512)                  0         ['activation_41[0][0]']       \n",
            " 3 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " global_max_pooling2d_13 (G  (None, 512)                  0         ['activation_41[0][0]']       \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_52 (Dense)            (None, 32)                   16416     ['global_average_pooling2d_13[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_54 (Dense)            (None, 32)                   16416     ['global_max_pooling2d_13[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_53 (Dense)            (None, 512)                  16896     ['dense_52[0][0]']            \n",
            "                                                                                                  \n",
            " dense_55 (Dense)            (None, 512)                  16896     ['dense_54[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_26 (Reshape)        (None, 1, 1, 512)            0         ['dense_53[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_27 (Reshape)        (None, 1, 1, 512)            0         ['dense_55[0][0]']            \n",
            "                                                                                                  \n",
            " add_27 (Add)                (None, 1, 1, 512)            0         ['reshape_26[0][0]',          \n",
            "                                                                     'reshape_27[0][0]']          \n",
            "                                                                                                  \n",
            " activation_42 (Activation)  (None, 1, 1, 512)            0         ['add_27[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)      (None, 8, 5, 512)            0         ['activation_41[0][0]',       \n",
            "                                                                     'activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " res5_b_branch2a (Conv2D)    (None, 8, 5, 512)            2359808   ['multiply_13[0][0]']         \n",
            "                                                                                                  \n",
            " bn5_b_branch2a (BatchNorma  (None, 8, 5, 512)            2048      ['res5_b_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_43 (Activation)  (None, 8, 5, 512)            0         ['bn5_b_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res5_b_branch2b (Conv2D)    (None, 8, 5, 512)            2359808   ['activation_43[0][0]']       \n",
            "                                                                                                  \n",
            " res5_b_branch1 (Conv2D)     (None, 8, 5, 512)            262656    ['multiply_13[0][0]']         \n",
            "                                                                                                  \n",
            " bn5_b_branch2b (BatchNorma  (None, 8, 5, 512)            2048      ['res5_b_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn5_b_branch1 (BatchNormal  (None, 8, 5, 512)            2048      ['res5_b_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_28 (Add)                (None, 8, 5, 512)            0         ['bn5_b_branch2b[0][0]',      \n",
            "                                                                     'bn5_b_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_44 (Activation)  (None, 8, 5, 512)            0         ['add_28[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 512)                  0         ['activation_44[0][0]']       \n",
            " 4 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " global_max_pooling2d_14 (G  (None, 512)                  0         ['activation_44[0][0]']       \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_56 (Dense)            (None, 32)                   16416     ['global_average_pooling2d_14[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_58 (Dense)            (None, 32)                   16416     ['global_max_pooling2d_14[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_57 (Dense)            (None, 512)                  16896     ['dense_56[0][0]']            \n",
            "                                                                                                  \n",
            " dense_59 (Dense)            (None, 512)                  16896     ['dense_58[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_28 (Reshape)        (None, 1, 1, 512)            0         ['dense_57[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_29 (Reshape)        (None, 1, 1, 512)            0         ['dense_59[0][0]']            \n",
            "                                                                                                  \n",
            " add_29 (Add)                (None, 1, 1, 512)            0         ['reshape_28[0][0]',          \n",
            "                                                                     'reshape_29[0][0]']          \n",
            "                                                                                                  \n",
            " activation_45 (Activation)  (None, 1, 1, 512)            0         ['add_29[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)      (None, 8, 5, 512)            0         ['activation_44[0][0]',       \n",
            "                                                                     'activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " res5_c_branch2a (Conv2D)    (None, 8, 5, 512)            2359808   ['multiply_14[0][0]']         \n",
            "                                                                                                  \n",
            " bn5_c_branch2a (BatchNorma  (None, 8, 5, 512)            2048      ['res5_c_branch2a[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " activation_46 (Activation)  (None, 8, 5, 512)            0         ['bn5_c_branch2a[0][0]']      \n",
            "                                                                                                  \n",
            " res5_c_branch2b (Conv2D)    (None, 8, 5, 512)            2359808   ['activation_46[0][0]']       \n",
            "                                                                                                  \n",
            " res5_c_branch1 (Conv2D)     (None, 8, 5, 512)            262656    ['multiply_14[0][0]']         \n",
            "                                                                                                  \n",
            " bn5_c_branch2b (BatchNorma  (None, 8, 5, 512)            2048      ['res5_c_branch2b[0][0]']     \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " bn5_c_branch1 (BatchNormal  (None, 8, 5, 512)            2048      ['res5_c_branch1[0][0]']      \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " add_30 (Add)                (None, 8, 5, 512)            0         ['bn5_c_branch2b[0][0]',      \n",
            "                                                                     'bn5_c_branch1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_47 (Activation)  (None, 8, 5, 512)            0         ['add_30[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1  (None, 512)                  0         ['activation_47[0][0]']       \n",
            " 5 (GlobalAveragePooling2D)                                                                       \n",
            "                                                                                                  \n",
            " global_max_pooling2d_15 (G  (None, 512)                  0         ['activation_47[0][0]']       \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_60 (Dense)            (None, 32)                   16416     ['global_average_pooling2d_15[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dense_62 (Dense)            (None, 32)                   16416     ['global_max_pooling2d_15[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_61 (Dense)            (None, 512)                  16896     ['dense_60[0][0]']            \n",
            "                                                                                                  \n",
            " dense_63 (Dense)            (None, 512)                  16896     ['dense_62[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_30 (Reshape)        (None, 1, 1, 512)            0         ['dense_61[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_31 (Reshape)        (None, 1, 1, 512)            0         ['dense_63[0][0]']            \n",
            "                                                                                                  \n",
            " add_31 (Add)                (None, 1, 1, 512)            0         ['reshape_30[0][0]',          \n",
            "                                                                     'reshape_31[0][0]']          \n",
            "                                                                                                  \n",
            " activation_48 (Activation)  (None, 1, 1, 512)            0         ['add_31[0][0]']              \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)      (None, 8, 5, 512)            0         ['activation_47[0][0]',       \n",
            "                                                                     'activation_48[0][0]']       \n",
            "                                                                                                  \n",
            " max_pool (MaxPooling2D)     (None, 4, 2, 512)            0         ['multiply_15[0][0]']         \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 4096)                 0         ['max_pool[0][0]']            \n",
            "                                                                                                  \n",
            " fc60 (Dense)                (None, 60)                   245820    ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 22799956 (86.97 MB)\n",
            "Trainable params: 22777172 (86.89 MB)\n",
            "Non-trainable params: 22784 (89.00 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ly3YVkvLyjqc",
        "outputId": "05083b04-7db5-4ef8-dca4-9632b90288ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport tensorflow as tf\\nfrom keras.layers import Layer, Conv2D, MaxPooling2D, Flatten, Dense, Permute, Multiply\\nfrom keras.models import Sequential\\nfrom keras import backend as K\\n\\nclass AttentionLayer(Layer):\\n    def __init__(self, **kwargs):\\n        super(AttentionLayer, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):\\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\\n        super(AttentionLayer, self).build(input_shape)\\n\\ndef call(self, x):\\n    x_transpose = K.permute_dimensions(x, pattern=(0, 2, 1, 3))  # Transpose to (None, num_features, segment_length, 256)\\n    et = K.squeeze(K.tanh(K.dot(x_transpose, self.W) + self.b), axis=-1)\\n    at = K.softmax(et)\\n    at = K.expand_dims(at, axis=-1)\\n    output = x * at\\n    output = K.sum(output, axis=1)\\n\\n    # Reshape the output tensor to match the expected shape\\n    output = K.reshape(output, (-1, 18, 29, 256))  # Adjust the shape according to your data\\n\\n    return output\\n\\n\\n    def compute_output_shape(self, input_shape):\\n        return (input_shape[0], 18, 29, 256)  # Adjust the shape according to your data\\n\\ndef tiny_cnn_vgg19_modified_with_attention(num_classes, segment_length, num_features):\\n    input_shape = (segment_length, num_features, 1)  # Assuming grayscale images\\n\\n    model = Sequential()\\n\\n    # Convolutional layers\\n    model.add(Conv2D(64, (3, 3), activation=\\'relu\\', padding=\\'same\\', input_shape=input_shape))\\n    model.add(Conv2D(64, (3, 3), activation=\\'relu\\', padding=\\'same\\'))\\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\\n\\n    model.add(Conv2D(128, (3, 3), activation=\\'relu\\', padding=\\'same\\'))\\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\\n\\n    model.add(Conv2D(256, (3, 3), activation=\\'relu\\', padding=\\'same\\'))\\n    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\\n\\n    # Attention Layer\\n    model.add(Permute((2, 1, 3)))  # Transpose to (num_features, segment_length, 256)\\n    model.add(AttentionLayer())\\n\\n    # Flatten layer\\n    model.add(Flatten())\\n\\n    # Fully connected layers\\n    model.add(Dense(512, activation=\\'relu\\'))\\n    model.add(Dense(512, activation=\\'relu\\'))\\n    model.add(Dense(num_classes, activation=\\'softmax\\'))\\n\\n    return model\\n\\n# Assuming your input parameters\\nnum_classes = 60\\nsegment_length = 237\\nnum_features = 150\\n\\n# Create the modified tiny CNN VGG19-like model with attention\\nmodel_modified_with_attention = tiny_cnn_vgg19_modified_with_attention(num_classes, segment_length, num_features)\\n\\n# Print model architecture\\nprint(model_modified_with_attention.summary())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from keras.layers import Layer, Conv2D, MaxPooling2D, Flatten, Dense, Permute, Multiply\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "def call(self, x):\n",
        "    x_transpose = K.permute_dimensions(x, pattern=(0, 2, 1, 3))  # Transpose to (None, num_features, segment_length, 256)\n",
        "    et = K.squeeze(K.tanh(K.dot(x_transpose, self.W) + self.b), axis=-1)\n",
        "    at = K.softmax(et)\n",
        "    at = K.expand_dims(at, axis=-1)\n",
        "    output = x * at\n",
        "    output = K.sum(output, axis=1)\n",
        "\n",
        "    # Reshape the output tensor to match the expected shape\n",
        "    output = K.reshape(output, (-1, 18, 29, 256))  # Adjust the shape according to your data\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 18, 29, 256)  # Adjust the shape according to your data\n",
        "\n",
        "def tiny_cnn_vgg19_modified_with_attention(num_classes, segment_length, num_features):\n",
        "    input_shape = (segment_length, num_features, 1)  # Assuming grayscale images\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Attention Layer\n",
        "    model.add(Permute((2, 1, 3)))  # Transpose to (num_features, segment_length, 256)\n",
        "    model.add(AttentionLayer())\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming your input parameters\n",
        "num_classes = 60\n",
        "segment_length = 237\n",
        "num_features = 150\n",
        "\n",
        "# Create the modified tiny CNN VGG19-like model with attention\n",
        "model_modified_with_attention = tiny_cnn_vgg19_modified_with_attention(num_classes, segment_length, num_features)\n",
        "\n",
        "# Print model architecture\n",
        "print(model_modified_with_attention.summary())\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aL3Uvj5qybtR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zhanEADTmmOO"
      },
      "outputs": [],
      "source": [
        "# Set the parameters\n",
        "#num_segments = 1 # You can adjust this based on your requirements\n",
        "#segment_length = 237\n",
        "#num_features = 150  # Assuming each frame has 150 features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CdhHz2gvH1j",
        "outputId": "67f6f6c1-7c30-4416-dad1-85cdfbfc9e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1704, 237, 150)\n"
          ]
        }
      ],
      "source": [
        "print(x_train_CS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMvU4ewfviyc",
        "outputId": "9a6bb4a7-ba4b-4841-eb73-9f5eaaea12f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1704, 60)\n"
          ]
        }
      ],
      "source": [
        "print(y_train_CS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEXOZnVrvH1l",
        "outputId": "9999c178-4e37-435b-816d-720ae86f79bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 237, 150)\n"
          ]
        }
      ],
      "source": [
        "print(x_valid_CS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg4mFqQgvy8X",
        "outputId": "6e43852a-9d83-4156-d202-190b8d628f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 60)\n"
          ]
        }
      ],
      "source": [
        "print(y_valid_CS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhGdnMS9vH1m",
        "outputId": "b4036ff0-603f-4eac-dc1c-3cf84337a461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1078, 237, 150)\n"
          ]
        }
      ],
      "source": [
        "print(x_test_CS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHv-h3zNwTtk",
        "outputId": "885bc07a-a9c5-4467-8c92-8cc5bd4b037e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1078, 60)\n"
          ]
        }
      ],
      "source": [
        "print(y_test_CS.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRnRhPtkvH1l",
        "outputId": "3c58b155-3896-48a7-d7f0-53c7c1539508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1816, 237, 150)\n"
          ]
        }
      ],
      "source": [
        "print(x_train_CV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV1eFZO0vnKs",
        "outputId": "46077d80-9db4-4028-a12f-30da5cc1b774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1816, 60)\n"
          ]
        }
      ],
      "source": [
        "print(y_train_CV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ke5uMEawABY",
        "outputId": "91f037a4-a2c7-4482-f763-abfe1e0218e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 237, 150)\n"
          ]
        }
      ],
      "source": [
        "print(x_valid_CV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24QIYHRdwBMW",
        "outputId": "4efc96c6-629f-4e59-90d5-695d92313bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 60)\n"
          ]
        }
      ],
      "source": [
        "print(y_valid_CV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJNy30yuvbh6",
        "outputId": "e1bfb9f5-69a4-4006-8c76-60f7f8498f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(960, 237, 150)\n"
          ]
        }
      ],
      "source": [
        "print(x_test_CV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rynjwSPHwmYi",
        "outputId": "ca245a8f-91d0-4093-9ffb-17933f7f9c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(960, 60)\n"
          ]
        }
      ],
      "source": [
        "print(y_test_CV.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LDoDTAl1w4t6"
      },
      "outputs": [],
      "source": [
        "x_train_CS = np.reshape(x_train_CS, (1704, 237, 150))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d963S0JCynH-"
      },
      "outputs": [],
      "source": [
        "x_valid_CS = np.reshape(x_valid_CS, (90, 237, 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wxrXpGHoyn1B"
      },
      "outputs": [],
      "source": [
        "x_test_CS = np.reshape(x_test_CS, (1078,1, 237, 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "W6MWmuZp0DWw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-A9s7pRW0Drb"
      },
      "outputs": [],
      "source": [
        "x_train_CV = np.reshape(x_train_CV, (1816, 237, 150))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZTgfd8ym0Drc"
      },
      "outputs": [],
      "source": [
        "x_valid_CV = np.reshape(x_valid_CV, (96, 237, 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qYmAbwRM0Drc"
      },
      "outputs": [],
      "source": [
        "x_test_CV = np.reshape(x_test_CV, (960,1, 237, 150))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6oO_ycwpl4Cy"
      },
      "outputs": [],
      "source": [
        "# Define the Part-Aware LSTM model\n",
        "num_classes = 60  # Assuming there are 60 action classes\n",
        "multi_cnn_model_CS = EfficientNet_with_attention(input_shape, num_classes)\n",
        "multi_cnn_model_CV = EfficientNet_with_attention(input_shape, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-TUBRGf7mWX7"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "multi_cnn_model_CS.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "multi_cnn_model_CV.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M4A4gSrnFEhI"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "#multi_cnn_model_CS.fit(x_train_CS, y_train_CS, validation_data=(x_valid_CS, y_valid_CS), epochs=150, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NxJGPBnHFGUP",
        "outputId": "f34af4d5-8fe0-410b-defd-a272e6094ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "54/54 [==============================] - 72s 259ms/step - loss: 4.7180 - accuracy: 0.0293 - val_loss: 4.1430 - val_accuracy: 0.0111\n",
            "Epoch 2/250\n",
            "54/54 [==============================] - 10s 177ms/step - loss: 3.8527 - accuracy: 0.0622 - val_loss: 4.2473 - val_accuracy: 0.0222\n",
            "Epoch 3/250\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 3.4099 - accuracy: 0.1045 - val_loss: 4.6394 - val_accuracy: 0.0111\n",
            "Epoch 4/250\n",
            "54/54 [==============================] - 10s 177ms/step - loss: 3.1027 - accuracy: 0.1573 - val_loss: 4.8997 - val_accuracy: 0.0444\n",
            "Epoch 5/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 2.6008 - accuracy: 0.2254 - val_loss: 5.2244 - val_accuracy: 0.0111\n",
            "Epoch 6/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 2.3038 - accuracy: 0.2934 - val_loss: 5.6038 - val_accuracy: 0.0111\n",
            "Epoch 7/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 2.1090 - accuracy: 0.3433 - val_loss: 5.4876 - val_accuracy: 0.0111\n",
            "Epoch 8/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.8849 - accuracy: 0.4102 - val_loss: 6.7449 - val_accuracy: 0.0333\n",
            "Epoch 9/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.7070 - accuracy: 0.4742 - val_loss: 3.7036 - val_accuracy: 0.1444\n",
            "Epoch 10/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.5505 - accuracy: 0.4941 - val_loss: 2.9629 - val_accuracy: 0.2111\n",
            "Epoch 11/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 1.4193 - accuracy: 0.5464 - val_loss: 3.0163 - val_accuracy: 0.2778\n",
            "Epoch 12/250\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 1.2783 - accuracy: 0.5722 - val_loss: 2.4128 - val_accuracy: 0.3444\n",
            "Epoch 13/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 1.0665 - accuracy: 0.6379 - val_loss: 1.7379 - val_accuracy: 0.4889\n",
            "Epoch 14/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.0220 - accuracy: 0.6579 - val_loss: 2.4695 - val_accuracy: 0.4667\n",
            "Epoch 15/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.8694 - accuracy: 0.7171 - val_loss: 2.0245 - val_accuracy: 0.4889\n",
            "Epoch 16/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.7694 - accuracy: 0.7430 - val_loss: 2.2268 - val_accuracy: 0.5111\n",
            "Epoch 17/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.7030 - accuracy: 0.7482 - val_loss: 1.7790 - val_accuracy: 0.5222\n",
            "Epoch 18/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.6529 - accuracy: 0.7623 - val_loss: 1.9763 - val_accuracy: 0.5444\n",
            "Epoch 19/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.5544 - accuracy: 0.8016 - val_loss: 2.3272 - val_accuracy: 0.5667\n",
            "Epoch 20/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.5376 - accuracy: 0.8175 - val_loss: 2.4223 - val_accuracy: 0.5667\n",
            "Epoch 21/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.4239 - accuracy: 0.8580 - val_loss: 1.4943 - val_accuracy: 0.6778\n",
            "Epoch 22/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.5059 - accuracy: 0.8275 - val_loss: 2.7398 - val_accuracy: 0.4556\n",
            "Epoch 23/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.6057 - accuracy: 0.7999 - val_loss: 2.0967 - val_accuracy: 0.6667\n",
            "Epoch 24/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.3579 - accuracy: 0.8803 - val_loss: 1.6878 - val_accuracy: 0.6667\n",
            "Epoch 25/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.2440 - accuracy: 0.9173 - val_loss: 1.7760 - val_accuracy: 0.6778\n",
            "Epoch 26/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.3296 - accuracy: 0.8938 - val_loss: 2.0609 - val_accuracy: 0.6111\n",
            "Epoch 27/250\n",
            "54/54 [==============================] - 11s 203ms/step - loss: 0.1920 - accuracy: 0.9308 - val_loss: 1.4821 - val_accuracy: 0.6333\n",
            "Epoch 28/250\n",
            "54/54 [==============================] - 11s 200ms/step - loss: 0.2863 - accuracy: 0.9014 - val_loss: 1.5980 - val_accuracy: 0.6444\n",
            "Epoch 29/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.2559 - accuracy: 0.9161 - val_loss: 1.6015 - val_accuracy: 0.6333\n",
            "Epoch 30/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1705 - accuracy: 0.9454 - val_loss: 1.4669 - val_accuracy: 0.6667\n",
            "Epoch 31/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.1191 - accuracy: 0.9548 - val_loss: 1.1776 - val_accuracy: 0.7111\n",
            "Epoch 32/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.1817 - accuracy: 0.9366 - val_loss: 1.5387 - val_accuracy: 0.6889\n",
            "Epoch 33/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1307 - accuracy: 0.9572 - val_loss: 1.4259 - val_accuracy: 0.7333\n",
            "Epoch 34/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1000 - accuracy: 0.9707 - val_loss: 1.4931 - val_accuracy: 0.6667\n",
            "Epoch 35/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.2913 - accuracy: 0.9073 - val_loss: 1.8478 - val_accuracy: 0.5889\n",
            "Epoch 36/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1699 - accuracy: 0.9460 - val_loss: 1.4837 - val_accuracy: 0.6556\n",
            "Epoch 37/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1341 - accuracy: 0.9566 - val_loss: 1.5776 - val_accuracy: 0.7000\n",
            "Epoch 38/250\n",
            "54/54 [==============================] - 10s 191ms/step - loss: 0.1368 - accuracy: 0.9531 - val_loss: 1.8617 - val_accuracy: 0.6556\n",
            "Epoch 39/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.2019 - accuracy: 0.9372 - val_loss: 1.1815 - val_accuracy: 0.7111\n",
            "Epoch 40/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0661 - accuracy: 0.9771 - val_loss: 1.3940 - val_accuracy: 0.7000\n",
            "Epoch 41/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0629 - accuracy: 0.9812 - val_loss: 1.5456 - val_accuracy: 0.7000\n",
            "Epoch 42/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0958 - accuracy: 0.9718 - val_loss: 1.5240 - val_accuracy: 0.7000\n",
            "Epoch 43/250\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 0.0948 - accuracy: 0.9671 - val_loss: 1.2911 - val_accuracy: 0.7444\n",
            "Epoch 44/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1060 - accuracy: 0.9642 - val_loss: 1.1880 - val_accuracy: 0.7000\n",
            "Epoch 45/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1037 - accuracy: 0.9642 - val_loss: 1.2176 - val_accuracy: 0.7222\n",
            "Epoch 46/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0778 - accuracy: 0.9736 - val_loss: 1.7120 - val_accuracy: 0.7000\n",
            "Epoch 47/250\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 0.1868 - accuracy: 0.9442 - val_loss: 2.0632 - val_accuracy: 0.6222\n",
            "Epoch 48/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.1173 - accuracy: 0.9683 - val_loss: 2.0584 - val_accuracy: 0.6333\n",
            "Epoch 49/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0850 - accuracy: 0.9742 - val_loss: 1.7060 - val_accuracy: 0.7000\n",
            "Epoch 50/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 0.1089 - accuracy: 0.9695 - val_loss: 1.5059 - val_accuracy: 0.6333\n",
            "Epoch 51/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.1187 - accuracy: 0.9619 - val_loss: 2.0506 - val_accuracy: 0.6111\n",
            "Epoch 52/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1136 - accuracy: 0.9648 - val_loss: 1.6008 - val_accuracy: 0.6333\n",
            "Epoch 53/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1655 - accuracy: 0.9478 - val_loss: 1.7683 - val_accuracy: 0.6667\n",
            "Epoch 54/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0999 - accuracy: 0.9677 - val_loss: 1.5954 - val_accuracy: 0.6444\n",
            "Epoch 55/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1684 - accuracy: 0.9454 - val_loss: 2.9133 - val_accuracy: 0.6333\n",
            "Epoch 56/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1280 - accuracy: 0.9577 - val_loss: 1.8531 - val_accuracy: 0.6667\n",
            "Epoch 57/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.2071 - accuracy: 0.9337 - val_loss: 1.3120 - val_accuracy: 0.7111\n",
            "Epoch 58/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0507 - accuracy: 0.9859 - val_loss: 1.4886 - val_accuracy: 0.7222\n",
            "Epoch 59/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0713 - accuracy: 0.9800 - val_loss: 1.8530 - val_accuracy: 0.7000\n",
            "Epoch 60/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0385 - accuracy: 0.9877 - val_loss: 1.4962 - val_accuracy: 0.6889\n",
            "Epoch 61/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0246 - accuracy: 0.9924 - val_loss: 1.5777 - val_accuracy: 0.6667\n",
            "Epoch 62/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0249 - accuracy: 0.9959 - val_loss: 1.6140 - val_accuracy: 0.7444\n",
            "Epoch 63/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0107 - accuracy: 0.9971 - val_loss: 1.2597 - val_accuracy: 0.7889\n",
            "Epoch 64/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 1.1719 - val_accuracy: 0.7667\n",
            "Epoch 65/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 1.8528 - val_accuracy: 0.6556\n",
            "Epoch 66/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0221 - accuracy: 0.9935 - val_loss: 1.5747 - val_accuracy: 0.7333\n",
            "Epoch 67/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0592 - accuracy: 0.9830 - val_loss: 1.4283 - val_accuracy: 0.7444\n",
            "Epoch 68/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0406 - accuracy: 0.9877 - val_loss: 1.2966 - val_accuracy: 0.7667\n",
            "Epoch 69/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.2083 - accuracy: 0.9484 - val_loss: 1.6836 - val_accuracy: 0.6889\n",
            "Epoch 70/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.1457 - accuracy: 0.9519 - val_loss: 1.2145 - val_accuracy: 0.6889\n",
            "Epoch 71/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0577 - accuracy: 0.9842 - val_loss: 1.3934 - val_accuracy: 0.7667\n",
            "Epoch 72/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0290 - accuracy: 0.9894 - val_loss: 1.4663 - val_accuracy: 0.7000\n",
            "Epoch 73/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1226 - accuracy: 0.9648 - val_loss: 1.8410 - val_accuracy: 0.6667\n",
            "Epoch 74/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0924 - accuracy: 0.9648 - val_loss: 1.3662 - val_accuracy: 0.7222\n",
            "Epoch 75/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0418 - accuracy: 0.9894 - val_loss: 1.6825 - val_accuracy: 0.6222\n",
            "Epoch 76/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0501 - accuracy: 0.9859 - val_loss: 1.2350 - val_accuracy: 0.7333\n",
            "Epoch 77/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 1.3647 - val_accuracy: 0.7333\n",
            "Epoch 78/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0668 - accuracy: 0.9812 - val_loss: 1.9852 - val_accuracy: 0.6889\n",
            "Epoch 79/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1686 - accuracy: 0.9525 - val_loss: 2.1592 - val_accuracy: 0.6111\n",
            "Epoch 80/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1462 - accuracy: 0.9548 - val_loss: 1.6857 - val_accuracy: 0.6778\n",
            "Epoch 81/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 0.0622 - accuracy: 0.9800 - val_loss: 1.3592 - val_accuracy: 0.7000\n",
            "Epoch 82/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0524 - accuracy: 0.9836 - val_loss: 1.8144 - val_accuracy: 0.7111\n",
            "Epoch 83/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1081 - accuracy: 0.9718 - val_loss: 1.1770 - val_accuracy: 0.7222\n",
            "Epoch 84/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0327 - accuracy: 0.9888 - val_loss: 1.2911 - val_accuracy: 0.7556\n",
            "Epoch 85/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0374 - accuracy: 0.9906 - val_loss: 1.3245 - val_accuracy: 0.6889\n",
            "Epoch 86/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0641 - accuracy: 0.9818 - val_loss: 1.6914 - val_accuracy: 0.6778\n",
            "Epoch 87/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0510 - accuracy: 0.9877 - val_loss: 1.2945 - val_accuracy: 0.7556\n",
            "Epoch 88/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0117 - accuracy: 0.9977 - val_loss: 1.0903 - val_accuracy: 0.8111\n",
            "Epoch 89/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0038 - accuracy: 0.9982 - val_loss: 0.9942 - val_accuracy: 0.8222\n",
            "Epoch 90/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0375 - accuracy: 0.9894 - val_loss: 1.2232 - val_accuracy: 0.7000\n",
            "Epoch 91/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0353 - accuracy: 0.9888 - val_loss: 1.3123 - val_accuracy: 0.7333\n",
            "Epoch 92/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 1.5699 - val_accuracy: 0.7111\n",
            "Epoch 93/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0476 - accuracy: 0.9859 - val_loss: 1.2948 - val_accuracy: 0.7667\n",
            "Epoch 94/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0527 - accuracy: 0.9806 - val_loss: 1.6628 - val_accuracy: 0.6889\n",
            "Epoch 95/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0990 - accuracy: 0.9754 - val_loss: 1.3446 - val_accuracy: 0.7000\n",
            "Epoch 96/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 1.1794 - val_accuracy: 0.7889\n",
            "Epoch 97/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 1.1824 - val_accuracy: 0.7111\n",
            "Epoch 98/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0055 - accuracy: 0.9994 - val_loss: 1.0698 - val_accuracy: 0.7778\n",
            "Epoch 99/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0030 - accuracy: 0.9988 - val_loss: 1.1766 - val_accuracy: 0.7667\n",
            "Epoch 100/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0097 - accuracy: 0.9977 - val_loss: 1.2260 - val_accuracy: 0.7667\n",
            "Epoch 101/250\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 0.0493 - accuracy: 0.9871 - val_loss: 1.3854 - val_accuracy: 0.7222\n",
            "Epoch 102/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1572 - accuracy: 0.9577 - val_loss: 1.5564 - val_accuracy: 0.7222\n",
            "Epoch 103/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0654 - accuracy: 0.9812 - val_loss: 1.0771 - val_accuracy: 0.7667\n",
            "Epoch 104/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 0.1198 - accuracy: 0.9642 - val_loss: 1.2828 - val_accuracy: 0.7222\n",
            "Epoch 105/250\n",
            "54/54 [==============================] - 10s 179ms/step - loss: 0.0695 - accuracy: 0.9742 - val_loss: 1.4647 - val_accuracy: 0.7222\n",
            "Epoch 106/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0623 - accuracy: 0.9824 - val_loss: 1.3280 - val_accuracy: 0.7222\n",
            "Epoch 107/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1839 - accuracy: 0.9425 - val_loss: 1.6123 - val_accuracy: 0.6889\n",
            "Epoch 108/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0316 - accuracy: 0.9906 - val_loss: 1.1169 - val_accuracy: 0.7333\n",
            "Epoch 109/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0159 - accuracy: 0.9959 - val_loss: 0.9836 - val_accuracy: 0.7778\n",
            "Epoch 110/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0030 - accuracy: 0.9988 - val_loss: 1.0114 - val_accuracy: 0.8111\n",
            "Epoch 111/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0191 - accuracy: 0.9953 - val_loss: 1.0768 - val_accuracy: 0.7444\n",
            "Epoch 112/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 1.1180 - val_accuracy: 0.7556\n",
            "Epoch 113/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0511 - val_accuracy: 0.7889\n",
            "Epoch 114/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 5.2148e-04 - accuracy: 1.0000 - val_loss: 1.0448 - val_accuracy: 0.8000\n",
            "Epoch 115/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 2.8067e-04 - accuracy: 1.0000 - val_loss: 1.0531 - val_accuracy: 0.8111\n",
            "Epoch 116/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 2.5412e-04 - accuracy: 1.0000 - val_loss: 1.0661 - val_accuracy: 0.8111\n",
            "Epoch 117/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 1.3783e-04 - accuracy: 1.0000 - val_loss: 1.0823 - val_accuracy: 0.8111\n",
            "Epoch 118/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.8746e-04 - accuracy: 1.0000 - val_loss: 1.0873 - val_accuracy: 0.8000\n",
            "Epoch 119/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.2028e-04 - accuracy: 1.0000 - val_loss: 1.0948 - val_accuracy: 0.8000\n",
            "Epoch 120/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 1.1219 - val_accuracy: 0.7889\n",
            "Epoch 121/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.2959 - accuracy: 0.9167 - val_loss: 1.9151 - val_accuracy: 0.6222\n",
            "Epoch 122/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1980 - accuracy: 0.9425 - val_loss: 2.0214 - val_accuracy: 0.6444\n",
            "Epoch 123/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1198 - accuracy: 0.9613 - val_loss: 1.2708 - val_accuracy: 0.7222\n",
            "Epoch 124/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0806 - accuracy: 0.9789 - val_loss: 1.2034 - val_accuracy: 0.7333\n",
            "Epoch 125/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0970 - accuracy: 0.9677 - val_loss: 1.6179 - val_accuracy: 0.7444\n",
            "Epoch 126/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0501 - accuracy: 0.9859 - val_loss: 1.1739 - val_accuracy: 0.7333\n",
            "Epoch 127/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0262 - accuracy: 0.9924 - val_loss: 1.0662 - val_accuracy: 0.7889\n",
            "Epoch 128/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0307 - accuracy: 0.9918 - val_loss: 1.0883 - val_accuracy: 0.7778\n",
            "Epoch 129/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0786 - accuracy: 0.9800 - val_loss: 1.5239 - val_accuracy: 0.7222\n",
            "Epoch 130/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1080 - accuracy: 0.9712 - val_loss: 1.3289 - val_accuracy: 0.7111\n",
            "Epoch 131/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0545 - accuracy: 0.9859 - val_loss: 0.9977 - val_accuracy: 0.7667\n",
            "Epoch 132/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0683 - accuracy: 0.9795 - val_loss: 1.2596 - val_accuracy: 0.7333\n",
            "Epoch 133/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0362 - accuracy: 0.9918 - val_loss: 1.2606 - val_accuracy: 0.7556\n",
            "Epoch 134/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 1.1222 - val_accuracy: 0.7222\n",
            "Epoch 135/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0169 - accuracy: 0.9953 - val_loss: 1.4184 - val_accuracy: 0.7444\n",
            "Epoch 136/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 1.3991 - val_accuracy: 0.7889\n",
            "Epoch 137/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.9383 - val_accuracy: 0.8000\n",
            "Epoch 138/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0118 - accuracy: 0.9977 - val_loss: 0.9281 - val_accuracy: 0.7889\n",
            "Epoch 139/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 0.0177 - accuracy: 0.9959 - val_loss: 1.2224 - val_accuracy: 0.7333\n",
            "Epoch 140/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0443 - val_accuracy: 0.7778\n",
            "Epoch 141/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 3.8672e-04 - accuracy: 1.0000 - val_loss: 1.0283 - val_accuracy: 0.7778\n",
            "Epoch 142/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 2.7047e-04 - accuracy: 1.0000 - val_loss: 1.0156 - val_accuracy: 0.7778\n",
            "Epoch 143/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 5.8687e-04 - accuracy: 1.0000 - val_loss: 1.0957 - val_accuracy: 0.7556\n",
            "Epoch 144/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 9.9607e-04 - accuracy: 0.9994 - val_loss: 1.0329 - val_accuracy: 0.7778\n",
            "Epoch 145/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 1.1293 - val_accuracy: 0.7556\n",
            "Epoch 146/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 1.1757 - val_accuracy: 0.7556\n",
            "Epoch 147/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 1.1377 - val_accuracy: 0.7556\n",
            "Epoch 148/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0445 - accuracy: 0.9935 - val_loss: 1.3366 - val_accuracy: 0.7444\n",
            "Epoch 149/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0320 - accuracy: 0.9871 - val_loss: 1.0998 - val_accuracy: 0.7667\n",
            "Epoch 150/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0765 - accuracy: 0.9771 - val_loss: 1.2873 - val_accuracy: 0.7889\n",
            "Epoch 151/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 0.1077 - accuracy: 0.9683 - val_loss: 1.5934 - val_accuracy: 0.7222\n",
            "Epoch 152/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1584 - accuracy: 0.9484 - val_loss: 5.4999 - val_accuracy: 0.4111\n",
            "Epoch 153/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.1050 - accuracy: 0.9683 - val_loss: 1.1307 - val_accuracy: 0.8111\n",
            "Epoch 154/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0391 - accuracy: 0.9865 - val_loss: 1.6080 - val_accuracy: 0.7333\n",
            "Epoch 155/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0742 - accuracy: 0.9748 - val_loss: 1.3965 - val_accuracy: 0.7778\n",
            "Epoch 156/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1385 - accuracy: 0.9560 - val_loss: 0.9501 - val_accuracy: 0.8000\n",
            "Epoch 157/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0431 - accuracy: 0.9883 - val_loss: 1.0574 - val_accuracy: 0.7556\n",
            "Epoch 158/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0316 - accuracy: 0.9906 - val_loss: 0.9948 - val_accuracy: 0.7778\n",
            "Epoch 159/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0281 - accuracy: 0.9900 - val_loss: 0.9547 - val_accuracy: 0.8111\n",
            "Epoch 160/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0445 - accuracy: 0.9912 - val_loss: 1.3062 - val_accuracy: 0.7556\n",
            "Epoch 161/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0340 - accuracy: 0.9912 - val_loss: 1.6200 - val_accuracy: 0.7333\n",
            "Epoch 162/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0308 - accuracy: 0.9894 - val_loss: 1.2317 - val_accuracy: 0.7556\n",
            "Epoch 163/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 1.2879 - val_accuracy: 0.7667\n",
            "Epoch 164/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.1190 - accuracy: 0.9660 - val_loss: 0.9951 - val_accuracy: 0.8000\n",
            "Epoch 165/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0280 - accuracy: 0.9918 - val_loss: 0.7852 - val_accuracy: 0.8444\n",
            "Epoch 166/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0161 - accuracy: 0.9953 - val_loss: 1.0002 - val_accuracy: 0.8111\n",
            "Epoch 167/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 1.1124 - val_accuracy: 0.7889\n",
            "Epoch 168/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 1.0306 - val_accuracy: 0.8000\n",
            "Epoch 169/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 5.5602e-04 - accuracy: 1.0000 - val_loss: 1.0317 - val_accuracy: 0.8000\n",
            "Epoch 170/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 3.1394e-04 - accuracy: 1.0000 - val_loss: 1.0307 - val_accuracy: 0.8111\n",
            "Epoch 171/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 3.3154e-04 - accuracy: 1.0000 - val_loss: 1.0599 - val_accuracy: 0.8222\n",
            "Epoch 172/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 1.5986e-04 - accuracy: 1.0000 - val_loss: 1.0743 - val_accuracy: 0.8222\n",
            "Epoch 173/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.2560e-04 - accuracy: 1.0000 - val_loss: 1.0689 - val_accuracy: 0.8111\n",
            "Epoch 174/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 9.4395e-05 - accuracy: 1.0000 - val_loss: 1.0690 - val_accuracy: 0.8111\n",
            "Epoch 175/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 9.7740e-05 - accuracy: 1.0000 - val_loss: 1.0704 - val_accuracy: 0.8111\n",
            "Epoch 176/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.0977e-04 - accuracy: 1.0000 - val_loss: 1.0592 - val_accuracy: 0.8111\n",
            "Epoch 177/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 7.5430e-05 - accuracy: 1.0000 - val_loss: 1.0612 - val_accuracy: 0.8111\n",
            "Epoch 178/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 8.1253e-05 - accuracy: 1.0000 - val_loss: 1.0660 - val_accuracy: 0.8111\n",
            "Epoch 179/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 4.9848e-05 - accuracy: 1.0000 - val_loss: 1.0616 - val_accuracy: 0.8111\n",
            "Epoch 180/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.3353e-04 - accuracy: 1.0000 - val_loss: 1.0869 - val_accuracy: 0.8000\n",
            "Epoch 181/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 4.4090e-05 - accuracy: 1.0000 - val_loss: 1.0832 - val_accuracy: 0.8000\n",
            "Epoch 182/250\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 4.6788e-05 - accuracy: 1.0000 - val_loss: 1.0808 - val_accuracy: 0.8000\n",
            "Epoch 183/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 5.3518e-05 - accuracy: 1.0000 - val_loss: 1.0773 - val_accuracy: 0.8111\n",
            "Epoch 184/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 3.5199e-05 - accuracy: 1.0000 - val_loss: 1.0658 - val_accuracy: 0.8111\n",
            "Epoch 185/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 3.6603e-05 - accuracy: 1.0000 - val_loss: 1.0649 - val_accuracy: 0.8111\n",
            "Epoch 186/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 4.5703e-05 - accuracy: 1.0000 - val_loss: 1.0720 - val_accuracy: 0.8111\n",
            "Epoch 187/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 3.2639e-05 - accuracy: 1.0000 - val_loss: 1.0583 - val_accuracy: 0.8222\n",
            "Epoch 188/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 4.0186e-05 - accuracy: 1.0000 - val_loss: 1.0577 - val_accuracy: 0.8111\n",
            "Epoch 189/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 3.0403e-05 - accuracy: 1.0000 - val_loss: 1.0627 - val_accuracy: 0.8111\n",
            "Epoch 190/250\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 3.5219e-05 - accuracy: 1.0000 - val_loss: 1.0660 - val_accuracy: 0.8111\n",
            "Epoch 191/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 2.5476e-05 - accuracy: 1.0000 - val_loss: 1.0671 - val_accuracy: 0.8111\n",
            "Epoch 192/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 2.5120e-05 - accuracy: 1.0000 - val_loss: 1.0709 - val_accuracy: 0.8111\n",
            "Epoch 193/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 3.1963e-05 - accuracy: 1.0000 - val_loss: 1.0743 - val_accuracy: 0.8000\n",
            "Epoch 194/250\n",
            "54/54 [==============================] - 10s 187ms/step - loss: 2.2898e-05 - accuracy: 1.0000 - val_loss: 1.0780 - val_accuracy: 0.8000\n",
            "Epoch 195/250\n",
            "54/54 [==============================] - 10s 180ms/step - loss: 2.6365e-05 - accuracy: 1.0000 - val_loss: 1.0812 - val_accuracy: 0.8000\n",
            "Epoch 196/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 2.0076e-05 - accuracy: 1.0000 - val_loss: 1.0777 - val_accuracy: 0.8000\n",
            "Epoch 197/250\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 2.9478e-05 - accuracy: 1.0000 - val_loss: 1.0746 - val_accuracy: 0.8000\n",
            "Epoch 198/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 2.0913e-05 - accuracy: 1.0000 - val_loss: 1.0661 - val_accuracy: 0.8111\n",
            "Epoch 199/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 2.9023e-05 - accuracy: 1.0000 - val_loss: 1.0646 - val_accuracy: 0.8111\n",
            "Epoch 200/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 2.7378e-05 - accuracy: 1.0000 - val_loss: 1.0653 - val_accuracy: 0.8111\n",
            "Epoch 201/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.8125e-05 - accuracy: 1.0000 - val_loss: 1.0692 - val_accuracy: 0.8111\n",
            "Epoch 202/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 2.2430e-05 - accuracy: 1.0000 - val_loss: 1.0747 - val_accuracy: 0.8111\n",
            "Epoch 203/250\n",
            "54/54 [==============================] - 10s 181ms/step - loss: 3.2871e-05 - accuracy: 1.0000 - val_loss: 1.0839 - val_accuracy: 0.8111\n",
            "Epoch 204/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 9.8699e-05 - accuracy: 1.0000 - val_loss: 1.0712 - val_accuracy: 0.8222\n",
            "Epoch 205/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 1.8584 - val_accuracy: 0.7111\n",
            "Epoch 206/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.4722 - accuracy: 0.8556 - val_loss: 14.2863 - val_accuracy: 0.2111\n",
            "Epoch 207/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.2848 - accuracy: 0.9149 - val_loss: 5.4745 - val_accuracy: 0.5000\n",
            "Epoch 208/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0905 - accuracy: 0.9712 - val_loss: 1.4686 - val_accuracy: 0.7778\n",
            "Epoch 209/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0499 - accuracy: 0.9859 - val_loss: 1.3339 - val_accuracy: 0.7444\n",
            "Epoch 210/250\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 0.0485 - accuracy: 0.9877 - val_loss: 1.2450 - val_accuracy: 0.7444\n",
            "Epoch 211/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0385 - accuracy: 0.9871 - val_loss: 1.5614 - val_accuracy: 0.7444\n",
            "Epoch 212/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0154 - accuracy: 0.9959 - val_loss: 1.4746 - val_accuracy: 0.7444\n",
            "Epoch 213/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 1.1304 - val_accuracy: 0.7778\n",
            "Epoch 214/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0351 - accuracy: 0.9906 - val_loss: 1.3252 - val_accuracy: 0.7889\n",
            "Epoch 215/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0714 - accuracy: 0.9783 - val_loss: 1.0791 - val_accuracy: 0.7889\n",
            "Epoch 216/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0166 - accuracy: 0.9959 - val_loss: 0.9755 - val_accuracy: 0.7556\n",
            "Epoch 217/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0114 - accuracy: 0.9971 - val_loss: 1.0332 - val_accuracy: 0.7556\n",
            "Epoch 218/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 1.1383 - val_accuracy: 0.8111\n",
            "Epoch 219/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0267 - accuracy: 0.9953 - val_loss: 1.0466 - val_accuracy: 0.7444\n",
            "Epoch 220/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0326 - accuracy: 0.9924 - val_loss: 1.0262 - val_accuracy: 0.8222\n",
            "Epoch 221/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8976 - val_accuracy: 0.8111\n",
            "Epoch 222/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 4.3877e-04 - accuracy: 1.0000 - val_loss: 0.8955 - val_accuracy: 0.8111\n",
            "Epoch 223/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.8131e-04 - accuracy: 1.0000 - val_loss: 0.8895 - val_accuracy: 0.8444\n",
            "Epoch 224/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 1.7114e-04 - accuracy: 1.0000 - val_loss: 0.8863 - val_accuracy: 0.8333\n",
            "Epoch 225/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 1.9986e-04 - accuracy: 1.0000 - val_loss: 0.8915 - val_accuracy: 0.8333\n",
            "Epoch 226/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 1.0743e-04 - accuracy: 1.0000 - val_loss: 0.8959 - val_accuracy: 0.8333\n",
            "Epoch 227/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 1.1255e-04 - accuracy: 1.0000 - val_loss: 0.8950 - val_accuracy: 0.8333\n",
            "Epoch 228/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 1.0518e-04 - accuracy: 1.0000 - val_loss: 0.8998 - val_accuracy: 0.8444\n",
            "Epoch 229/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 1.3248e-04 - accuracy: 1.0000 - val_loss: 0.9091 - val_accuracy: 0.8444\n",
            "Epoch 230/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 7.7618e-05 - accuracy: 1.0000 - val_loss: 0.9083 - val_accuracy: 0.8444\n",
            "Epoch 231/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 8.7922e-05 - accuracy: 1.0000 - val_loss: 0.9137 - val_accuracy: 0.8444\n",
            "Epoch 232/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 5.6860e-05 - accuracy: 1.0000 - val_loss: 0.9134 - val_accuracy: 0.8444\n",
            "Epoch 233/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 5.6006e-05 - accuracy: 1.0000 - val_loss: 0.9128 - val_accuracy: 0.8444\n",
            "Epoch 234/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 4.7199e-05 - accuracy: 1.0000 - val_loss: 0.9140 - val_accuracy: 0.8444\n",
            "Epoch 235/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 6.6966e-05 - accuracy: 1.0000 - val_loss: 0.9192 - val_accuracy: 0.8444\n",
            "Epoch 236/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 5.0575e-05 - accuracy: 1.0000 - val_loss: 0.9230 - val_accuracy: 0.8444\n",
            "Epoch 237/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 5.9030e-05 - accuracy: 1.0000 - val_loss: 0.9198 - val_accuracy: 0.8444\n",
            "Epoch 238/250\n",
            "54/54 [==============================] - 10s 185ms/step - loss: 4.4963e-05 - accuracy: 1.0000 - val_loss: 0.9180 - val_accuracy: 0.8444\n",
            "Epoch 239/250\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 3.4183e-05 - accuracy: 1.0000 - val_loss: 0.9197 - val_accuracy: 0.8444\n",
            "Epoch 240/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 3.8034e-05 - accuracy: 1.0000 - val_loss: 0.9272 - val_accuracy: 0.8444\n",
            "Epoch 241/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 5.6217e-05 - accuracy: 1.0000 - val_loss: 0.9258 - val_accuracy: 0.8444\n",
            "Epoch 242/250\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 2.8959e-05 - accuracy: 1.0000 - val_loss: 0.9280 - val_accuracy: 0.8444\n",
            "Epoch 243/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 2.6684e-05 - accuracy: 1.0000 - val_loss: 0.9301 - val_accuracy: 0.8444\n",
            "Epoch 244/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 2.8820e-05 - accuracy: 1.0000 - val_loss: 0.9327 - val_accuracy: 0.8444\n",
            "Epoch 245/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 4.9617e-05 - accuracy: 1.0000 - val_loss: 0.9358 - val_accuracy: 0.8444\n",
            "Epoch 246/250\n",
            "54/54 [==============================] - 10s 186ms/step - loss: 5.5746e-05 - accuracy: 1.0000 - val_loss: 0.9414 - val_accuracy: 0.8444\n",
            "Epoch 247/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 4.4158e-05 - accuracy: 1.0000 - val_loss: 0.9879 - val_accuracy: 0.8444\n",
            "Epoch 248/250\n",
            "54/54 [==============================] - 10s 183ms/step - loss: 3.3669e-05 - accuracy: 1.0000 - val_loss: 0.9836 - val_accuracy: 0.8444\n",
            "Epoch 249/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 3.1839e-05 - accuracy: 1.0000 - val_loss: 0.9885 - val_accuracy: 0.8444\n",
            "Epoch 250/250\n",
            "54/54 [==============================] - 10s 184ms/step - loss: 2.3682e-05 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.8444\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have already trained your model and have a `history` object\n",
        "history = multi_cnn_model_CS.fit(x_train_CS, y_train_CS, epochs=250, batch_size=32, validation_data=(x_valid_CS, y_valid_CS))\n",
        "\n",
        "# Plot training vs validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_CS = np.reshape(x_test_CS, (1078, 237, 150, 1))"
      ],
      "metadata": {
        "id": "x0lfD9U7vyvn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict classes using the test data\n",
        "predictions_CS = multi_cnn_model_CS.predict(x_test_CS)\n",
        "predicted_classes_CS = np.argmax(predictions_CS, axis=1)\n"
      ],
      "metadata": {
        "id": "fOpw0QYSlU-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d3fde4-fdd3-462b-b3ce-98232ec03d72"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34/34 [==============================] - 4s 73ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# True classes\n",
        "true_classes_CS = np.argmax(y_test_CS, axis=1)"
      ],
      "metadata": {
        "id": "j-jEuXBm17ZH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare predicted classes with true classes and calculate accuracy\n",
        "accuracy_CS = np.mean(predicted_classes_CS == true_classes_CS)\n",
        "print(\"Accuracy (CS):\", accuracy_CS)"
      ],
      "metadata": {
        "id": "du-sLoWmlltc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96179ccc-c6e0-4ff4-9080-552a744a8eb2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (CS): 0.45454545454545453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some of the predicted and true classes\n",
        "print(\"Predicted Classes (CS):\", predicted_classes_CS[:10])\n",
        "print(\"True Classes (CS):\", true_classes_CS[:10])"
      ],
      "metadata": {
        "id": "9oCPIjf4lovn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44da8ac8-9138-4c07-db2d-e52dd6236c0a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Classes (CS): [ 0 10  2  3 42 41  6 15 58  9]\n",
            "True Classes (CS): [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Q4FChbxACNus"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "#multi_cnn_model_CV.fit(x_train_CV, y_train_CV, validation_data=(x_valid_CV, y_valid_CV), epochs=150, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn_keRvYFH_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e763a910-dd83-407a-acf9-15c06fa48ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "57/57 [==============================] - 65s 254ms/step - loss: 4.6711 - accuracy: 0.0380 - val_loss: 4.1026 - val_accuracy: 0.0625\n",
            "Epoch 2/250\n",
            "57/57 [==============================] - 11s 189ms/step - loss: 3.7541 - accuracy: 0.0732 - val_loss: 4.3448 - val_accuracy: 0.0104\n",
            "Epoch 3/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 3.1671 - accuracy: 0.1410 - val_loss: 5.0136 - val_accuracy: 0.0208\n",
            "Epoch 4/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 2.7676 - accuracy: 0.2098 - val_loss: 5.4471 - val_accuracy: 0.0104\n",
            "Epoch 5/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 2.4372 - accuracy: 0.2704 - val_loss: 6.7425 - val_accuracy: 0.0208\n",
            "Epoch 6/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 2.1312 - accuracy: 0.3464 - val_loss: 7.3553 - val_accuracy: 0.0208\n",
            "Epoch 7/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 2.0073 - accuracy: 0.3678 - val_loss: 5.1330 - val_accuracy: 0.0208\n",
            "Epoch 8/250\n",
            "57/57 [==============================] - 10s 182ms/step - loss: 1.8428 - accuracy: 0.4152 - val_loss: 4.2589 - val_accuracy: 0.1354\n",
            "Epoch 9/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 1.7454 - accuracy: 0.4433 - val_loss: 3.2655 - val_accuracy: 0.2292\n",
            "Epoch 10/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.5730 - accuracy: 0.5050 - val_loss: 2.5239 - val_accuracy: 0.3229\n",
            "Epoch 11/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.4547 - accuracy: 0.5182 - val_loss: 1.9882 - val_accuracy: 0.4583\n",
            "Epoch 12/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.2809 - accuracy: 0.5837 - val_loss: 2.5464 - val_accuracy: 0.4792\n",
            "Epoch 13/250\n",
            "57/57 [==============================] - 10s 182ms/step - loss: 1.2716 - accuracy: 0.5776 - val_loss: 1.9909 - val_accuracy: 0.5000\n",
            "Epoch 14/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 1.0817 - accuracy: 0.6311 - val_loss: 1.5326 - val_accuracy: 0.5938\n",
            "Epoch 15/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 1.0438 - accuracy: 0.6525 - val_loss: 3.3375 - val_accuracy: 0.3542\n",
            "Epoch 16/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.0015 - accuracy: 0.6713 - val_loss: 2.8009 - val_accuracy: 0.4062\n",
            "Epoch 17/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.8821 - accuracy: 0.6955 - val_loss: 2.0195 - val_accuracy: 0.5312\n",
            "Epoch 18/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.7901 - accuracy: 0.7412 - val_loss: 1.4799 - val_accuracy: 0.5833\n",
            "Epoch 19/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.6796 - accuracy: 0.7577 - val_loss: 1.4499 - val_accuracy: 0.6042\n",
            "Epoch 20/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.6275 - accuracy: 0.7902 - val_loss: 1.7724 - val_accuracy: 0.4688\n",
            "Epoch 21/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.6248 - accuracy: 0.7836 - val_loss: 2.5223 - val_accuracy: 0.4479\n",
            "Epoch 22/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.6318 - accuracy: 0.7863 - val_loss: 1.7200 - val_accuracy: 0.6146\n",
            "Epoch 23/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.5972 - accuracy: 0.7902 - val_loss: 1.5586 - val_accuracy: 0.5833\n",
            "Epoch 24/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.4353 - accuracy: 0.8552 - val_loss: 1.4287 - val_accuracy: 0.6146\n",
            "Epoch 25/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.4715 - accuracy: 0.8376 - val_loss: 1.4710 - val_accuracy: 0.6771\n",
            "Epoch 26/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.4131 - accuracy: 0.8629 - val_loss: 1.6674 - val_accuracy: 0.6042\n",
            "Epoch 27/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.3285 - accuracy: 0.8849 - val_loss: 1.8699 - val_accuracy: 0.5417\n",
            "Epoch 28/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.3080 - accuracy: 0.8948 - val_loss: 1.6113 - val_accuracy: 0.6562\n",
            "Epoch 29/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.3557 - accuracy: 0.8844 - val_loss: 1.5914 - val_accuracy: 0.6458\n",
            "Epoch 30/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.3270 - accuracy: 0.8943 - val_loss: 2.2041 - val_accuracy: 0.6146\n",
            "Epoch 31/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.2102 - accuracy: 0.9345 - val_loss: 1.4846 - val_accuracy: 0.6250\n",
            "Epoch 32/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.2603 - accuracy: 0.9141 - val_loss: 1.8727 - val_accuracy: 0.5417\n",
            "Epoch 33/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.2599 - accuracy: 0.9157 - val_loss: 1.5291 - val_accuracy: 0.6042\n",
            "Epoch 34/250\n",
            "57/57 [==============================] - 11s 188ms/step - loss: 0.2523 - accuracy: 0.9130 - val_loss: 1.5668 - val_accuracy: 0.6667\n",
            "Epoch 35/250\n",
            "57/57 [==============================] - 11s 188ms/step - loss: 0.2790 - accuracy: 0.9020 - val_loss: 1.6811 - val_accuracy: 0.6458\n",
            "Epoch 36/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.1695 - accuracy: 0.9471 - val_loss: 1.7088 - val_accuracy: 0.6771\n",
            "Epoch 37/250\n",
            "57/57 [==============================] - 10s 182ms/step - loss: 0.1822 - accuracy: 0.9328 - val_loss: 1.8157 - val_accuracy: 0.6458\n",
            "Epoch 38/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.1636 - accuracy: 0.9433 - val_loss: 1.6264 - val_accuracy: 0.6458\n",
            "Epoch 39/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.0908 - accuracy: 0.9697 - val_loss: 1.4459 - val_accuracy: 0.7083\n",
            "Epoch 40/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.1372 - accuracy: 0.9543 - val_loss: 2.0574 - val_accuracy: 0.6354\n",
            "Epoch 41/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.1646 - accuracy: 0.9466 - val_loss: 1.4130 - val_accuracy: 0.6667\n",
            "Epoch 42/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.1201 - accuracy: 0.9609 - val_loss: 1.7658 - val_accuracy: 0.6562\n",
            "Epoch 43/250\n",
            "57/57 [==============================] - 11s 188ms/step - loss: 0.1405 - accuracy: 0.9504 - val_loss: 1.4843 - val_accuracy: 0.6979\n",
            "Epoch 44/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.1290 - accuracy: 0.9554 - val_loss: 1.4177 - val_accuracy: 0.6771\n",
            "Epoch 45/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.2016 - accuracy: 0.9378 - val_loss: 1.5496 - val_accuracy: 0.6667\n",
            "Epoch 46/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.1616 - accuracy: 0.9493 - val_loss: 1.6533 - val_accuracy: 0.6250\n",
            "Epoch 47/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.1538 - accuracy: 0.9466 - val_loss: 1.9915 - val_accuracy: 0.6250\n",
            "Epoch 48/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.1558 - accuracy: 0.9493 - val_loss: 1.6420 - val_accuracy: 0.6875\n",
            "Epoch 49/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.1316 - accuracy: 0.9609 - val_loss: 1.7746 - val_accuracy: 0.6667\n",
            "Epoch 50/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.1107 - accuracy: 0.9664 - val_loss: 1.5212 - val_accuracy: 0.6667\n",
            "Epoch 51/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.0621 - accuracy: 0.9840 - val_loss: 1.2235 - val_accuracy: 0.7292\n",
            "Epoch 52/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.0481 - accuracy: 0.9851 - val_loss: 1.3780 - val_accuracy: 0.6771\n",
            "Epoch 53/250\n",
            "57/57 [==============================] - 11s 188ms/step - loss: 0.0952 - accuracy: 0.9736 - val_loss: 1.7107 - val_accuracy: 0.6771\n",
            "Epoch 54/250\n",
            "57/57 [==============================] - 11s 188ms/step - loss: 0.0941 - accuracy: 0.9741 - val_loss: 1.5499 - val_accuracy: 0.6979\n",
            "Epoch 55/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.0972 - accuracy: 0.9664 - val_loss: 1.4597 - val_accuracy: 0.6875\n",
            "Epoch 56/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.1891 - accuracy: 0.9361 - val_loss: 1.6753 - val_accuracy: 0.6458\n",
            "Epoch 57/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.2849 - accuracy: 0.9102 - val_loss: 2.1246 - val_accuracy: 0.5521\n",
            "Epoch 58/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.1142 - accuracy: 0.9604 - val_loss: 1.5926 - val_accuracy: 0.6771\n",
            "Epoch 59/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0785 - accuracy: 0.9719 - val_loss: 1.6059 - val_accuracy: 0.7083\n",
            "Epoch 60/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.0994 - accuracy: 0.9692 - val_loss: 1.8959 - val_accuracy: 0.6458\n",
            "Epoch 61/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.1120 - accuracy: 0.9675 - val_loss: 1.4359 - val_accuracy: 0.6875\n",
            "Epoch 62/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0802 - accuracy: 0.9741 - val_loss: 1.2308 - val_accuracy: 0.7083\n",
            "Epoch 63/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0573 - accuracy: 0.9851 - val_loss: 1.3443 - val_accuracy: 0.6875\n",
            "Epoch 64/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0396 - accuracy: 0.9906 - val_loss: 1.5113 - val_accuracy: 0.7292\n",
            "Epoch 65/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.0265 - accuracy: 0.9901 - val_loss: 1.4552 - val_accuracy: 0.6875\n",
            "Epoch 66/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.0675 - accuracy: 0.9769 - val_loss: 1.5176 - val_accuracy: 0.7188\n",
            "Epoch 67/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0316 - accuracy: 0.9912 - val_loss: 1.4519 - val_accuracy: 0.7604\n",
            "Epoch 68/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.1124 - accuracy: 0.9708 - val_loss: 2.0652 - val_accuracy: 0.6354\n",
            "Epoch 69/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.1298 - accuracy: 0.9537 - val_loss: 1.4803 - val_accuracy: 0.7083\n",
            "Epoch 70/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0798 - accuracy: 0.9725 - val_loss: 1.3547 - val_accuracy: 0.7500\n",
            "Epoch 71/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.1915 - accuracy: 0.9361 - val_loss: 2.1692 - val_accuracy: 0.6250\n",
            "Epoch 72/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.2436 - accuracy: 0.9273 - val_loss: 1.9556 - val_accuracy: 0.6146\n",
            "Epoch 73/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.1001 - accuracy: 0.9692 - val_loss: 1.4970 - val_accuracy: 0.7083\n",
            "Epoch 74/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 0.1048 - accuracy: 0.9747 - val_loss: 1.6998 - val_accuracy: 0.6771\n",
            "Epoch 75/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.0960 - accuracy: 0.9730 - val_loss: 1.4064 - val_accuracy: 0.7188\n",
            "Epoch 76/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.0763 - accuracy: 0.9752 - val_loss: 1.4986 - val_accuracy: 0.7292\n",
            "Epoch 77/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0582 - accuracy: 0.9780 - val_loss: 1.4927 - val_accuracy: 0.7292\n",
            "Epoch 78/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.0376 - accuracy: 0.9862 - val_loss: 1.5714 - val_accuracy: 0.7396\n",
            "Epoch 79/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.0535 - accuracy: 0.9835 - val_loss: 1.7534 - val_accuracy: 0.6979\n",
            "Epoch 80/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.1122 - accuracy: 0.9736 - val_loss: 1.5832 - val_accuracy: 0.6875\n",
            "Epoch 81/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.0418 - accuracy: 0.9868 - val_loss: 1.7511 - val_accuracy: 0.7083\n",
            "Epoch 82/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.0291 - accuracy: 0.9906 - val_loss: 2.0682 - val_accuracy: 0.6875\n",
            "Epoch 83/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0237 - accuracy: 0.9939 - val_loss: 1.6351 - val_accuracy: 0.7604\n",
            "Epoch 84/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0276 - accuracy: 0.9928 - val_loss: 1.7803 - val_accuracy: 0.7188\n",
            "Epoch 85/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.0602 - accuracy: 0.9851 - val_loss: 1.5090 - val_accuracy: 0.7396\n",
            "Epoch 86/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 0.0462 - accuracy: 0.9868 - val_loss: 1.6509 - val_accuracy: 0.6667\n",
            "Epoch 87/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0491 - accuracy: 0.9868 - val_loss: 1.5240 - val_accuracy: 0.7500\n",
            "Epoch 88/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0959 - accuracy: 0.9686 - val_loss: 1.5556 - val_accuracy: 0.6458\n",
            "Epoch 89/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0492 - accuracy: 0.9829 - val_loss: 1.8262 - val_accuracy: 0.6458\n",
            "Epoch 90/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0624 - accuracy: 0.9818 - val_loss: 1.3867 - val_accuracy: 0.7292\n",
            "Epoch 91/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.1198 - accuracy: 0.9631 - val_loss: 2.4634 - val_accuracy: 0.5729\n",
            "Epoch 92/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.1410 - accuracy: 0.9532 - val_loss: 1.6324 - val_accuracy: 0.7292\n",
            "Epoch 93/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.1607 - accuracy: 0.9466 - val_loss: 1.7916 - val_accuracy: 0.6250\n",
            "Epoch 94/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 0.0858 - accuracy: 0.9763 - val_loss: 1.6860 - val_accuracy: 0.6875\n",
            "Epoch 95/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0712 - accuracy: 0.9796 - val_loss: 1.7447 - val_accuracy: 0.6771\n",
            "Epoch 96/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 0.0648 - accuracy: 0.9802 - val_loss: 1.4490 - val_accuracy: 0.6875\n",
            "Epoch 97/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0525 - accuracy: 0.9818 - val_loss: 1.7237 - val_accuracy: 0.6875\n",
            "Epoch 98/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0493 - accuracy: 0.9851 - val_loss: 1.6116 - val_accuracy: 0.7292\n",
            "Epoch 99/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0423 - accuracy: 0.9890 - val_loss: 1.5393 - val_accuracy: 0.7083\n",
            "Epoch 100/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0424 - accuracy: 0.9857 - val_loss: 1.3995 - val_accuracy: 0.7396\n",
            "Epoch 101/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0253 - accuracy: 0.9945 - val_loss: 1.3782 - val_accuracy: 0.7396\n",
            "Epoch 102/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0194 - accuracy: 0.9972 - val_loss: 1.4348 - val_accuracy: 0.7083\n",
            "Epoch 103/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.3904 - val_accuracy: 0.7500\n",
            "Epoch 104/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 7.4057e-04 - accuracy: 1.0000 - val_loss: 1.4395 - val_accuracy: 0.7292\n",
            "Epoch 105/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 0.0091 - accuracy: 0.9994 - val_loss: 1.5264 - val_accuracy: 0.7292\n",
            "Epoch 106/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 0.0088 - accuracy: 0.9989 - val_loss: 1.4247 - val_accuracy: 0.7604\n",
            "Epoch 107/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 6.4097e-04 - accuracy: 1.0000 - val_loss: 1.4178 - val_accuracy: 0.7500\n",
            "Epoch 108/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 4.1646e-04 - accuracy: 1.0000 - val_loss: 1.4287 - val_accuracy: 0.7604\n",
            "Epoch 109/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 2.3981e-04 - accuracy: 1.0000 - val_loss: 1.4483 - val_accuracy: 0.7500\n",
            "Epoch 110/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.9742e-04 - accuracy: 1.0000 - val_loss: 1.4581 - val_accuracy: 0.7708\n",
            "Epoch 111/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.3341e-04 - accuracy: 1.0000 - val_loss: 1.4817 - val_accuracy: 0.7708\n",
            "Epoch 112/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 1.6942e-04 - accuracy: 1.0000 - val_loss: 1.4812 - val_accuracy: 0.7708\n",
            "Epoch 113/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.1071e-04 - accuracy: 1.0000 - val_loss: 1.4919 - val_accuracy: 0.7708\n",
            "Epoch 114/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 8.3241e-05 - accuracy: 1.0000 - val_loss: 1.5017 - val_accuracy: 0.7708\n",
            "Epoch 115/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 9.0471e-05 - accuracy: 1.0000 - val_loss: 1.5158 - val_accuracy: 0.7708\n",
            "Epoch 116/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 7.5535e-05 - accuracy: 1.0000 - val_loss: 1.5299 - val_accuracy: 0.7604\n",
            "Epoch 117/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 8.1729e-05 - accuracy: 1.0000 - val_loss: 1.5413 - val_accuracy: 0.7604\n",
            "Epoch 118/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 5.2337e-05 - accuracy: 1.0000 - val_loss: 1.5490 - val_accuracy: 0.7500\n",
            "Epoch 119/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 8.5942e-05 - accuracy: 1.0000 - val_loss: 1.5622 - val_accuracy: 0.7604\n",
            "Epoch 120/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 5.4393e-05 - accuracy: 1.0000 - val_loss: 1.5701 - val_accuracy: 0.7604\n",
            "Epoch 121/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 5.1698e-05 - accuracy: 1.0000 - val_loss: 1.5782 - val_accuracy: 0.7604\n",
            "Epoch 122/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 4.7604e-05 - accuracy: 1.0000 - val_loss: 1.5846 - val_accuracy: 0.7604\n",
            "Epoch 123/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 4.8106e-05 - accuracy: 1.0000 - val_loss: 1.5907 - val_accuracy: 0.7500\n",
            "Epoch 124/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 4.4913e-05 - accuracy: 1.0000 - val_loss: 1.6003 - val_accuracy: 0.7500\n",
            "Epoch 125/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 3.5769e-05 - accuracy: 1.0000 - val_loss: 1.6088 - val_accuracy: 0.7500\n",
            "Epoch 126/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 3.6657e-05 - accuracy: 1.0000 - val_loss: 1.6160 - val_accuracy: 0.7500\n",
            "Epoch 127/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 1.3228e-04 - accuracy: 1.0000 - val_loss: 1.6369 - val_accuracy: 0.7396\n",
            "Epoch 128/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 5.5946e-05 - accuracy: 1.0000 - val_loss: 1.6408 - val_accuracy: 0.7396\n",
            "Epoch 129/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 3.2706e-05 - accuracy: 1.0000 - val_loss: 1.6486 - val_accuracy: 0.7396\n",
            "Epoch 130/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 1.4478e-04 - accuracy: 1.0000 - val_loss: 1.6709 - val_accuracy: 0.7292\n",
            "Epoch 131/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 7.4103e-05 - accuracy: 1.0000 - val_loss: 1.6445 - val_accuracy: 0.7396\n",
            "Epoch 132/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 4.2890e-05 - accuracy: 1.0000 - val_loss: 1.6525 - val_accuracy: 0.7396\n",
            "Epoch 133/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 4.3235e-05 - accuracy: 1.0000 - val_loss: 1.6704 - val_accuracy: 0.7396\n",
            "Epoch 134/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 5.3965e-05 - accuracy: 1.0000 - val_loss: 1.6806 - val_accuracy: 0.7396\n",
            "Epoch 135/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 4.7711e-05 - accuracy: 1.0000 - val_loss: 1.7033 - val_accuracy: 0.7396\n",
            "Epoch 136/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 2.2830e-05 - accuracy: 1.0000 - val_loss: 1.7204 - val_accuracy: 0.7396\n",
            "Epoch 137/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 3.3703e-05 - accuracy: 1.0000 - val_loss: 1.7181 - val_accuracy: 0.7396\n",
            "Epoch 138/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 2.0354e-05 - accuracy: 1.0000 - val_loss: 1.7262 - val_accuracy: 0.7396\n",
            "Epoch 139/250\n",
            "57/57 [==============================] - 11s 188ms/step - loss: 5.5942e-05 - accuracy: 1.0000 - val_loss: 1.7700 - val_accuracy: 0.7292\n",
            "Epoch 140/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 2.3686e-05 - accuracy: 1.0000 - val_loss: 1.7815 - val_accuracy: 0.7292\n",
            "Epoch 141/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 7.5761e-05 - accuracy: 1.0000 - val_loss: 1.8212 - val_accuracy: 0.7083\n",
            "Epoch 142/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 4.0072e-05 - accuracy: 1.0000 - val_loss: 1.7971 - val_accuracy: 0.7083\n",
            "Epoch 143/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 3.4023e-05 - accuracy: 1.0000 - val_loss: 1.7847 - val_accuracy: 0.7083\n",
            "Epoch 144/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.8821e-05 - accuracy: 1.0000 - val_loss: 1.7832 - val_accuracy: 0.7083\n",
            "Epoch 145/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.9511e-05 - accuracy: 1.0000 - val_loss: 1.7868 - val_accuracy: 0.7083\n",
            "Epoch 146/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.7891e-05 - accuracy: 1.0000 - val_loss: 1.7890 - val_accuracy: 0.7083\n",
            "Epoch 147/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 2.0321e-05 - accuracy: 1.0000 - val_loss: 1.7908 - val_accuracy: 0.7083\n",
            "Epoch 148/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.3682e-05 - accuracy: 1.0000 - val_loss: 1.7965 - val_accuracy: 0.7083\n",
            "Epoch 149/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.3903e-05 - accuracy: 1.0000 - val_loss: 1.7961 - val_accuracy: 0.7083\n",
            "Epoch 150/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.3431e-05 - accuracy: 1.0000 - val_loss: 1.8024 - val_accuracy: 0.7292\n",
            "Epoch 151/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 1.5254e-05 - accuracy: 1.0000 - val_loss: 1.8128 - val_accuracy: 0.7188\n",
            "Epoch 152/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.3687e-05 - accuracy: 1.0000 - val_loss: 1.8190 - val_accuracy: 0.7188\n",
            "Epoch 153/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 9.3516e-06 - accuracy: 1.0000 - val_loss: 1.8226 - val_accuracy: 0.7188\n",
            "Epoch 154/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 1.1802e-05 - accuracy: 1.0000 - val_loss: 1.8247 - val_accuracy: 0.7292\n",
            "Epoch 155/250\n",
            "57/57 [==============================] - 10s 183ms/step - loss: 1.1479e-05 - accuracy: 1.0000 - val_loss: 1.8284 - val_accuracy: 0.7292\n",
            "Epoch 156/250\n",
            "57/57 [==============================] - 11s 184ms/step - loss: 1.1033e-05 - accuracy: 1.0000 - val_loss: 1.8309 - val_accuracy: 0.7292\n",
            "Epoch 157/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 2.5240e-05 - accuracy: 1.0000 - val_loss: 1.8300 - val_accuracy: 0.7292\n",
            "Epoch 158/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 8.8806e-06 - accuracy: 1.0000 - val_loss: 1.8340 - val_accuracy: 0.7292\n",
            "Epoch 159/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 1.3047e-05 - accuracy: 1.0000 - val_loss: 1.8427 - val_accuracy: 0.7292\n",
            "Epoch 160/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 7.5177e-06 - accuracy: 1.0000 - val_loss: 1.8448 - val_accuracy: 0.7292\n",
            "Epoch 161/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 8.2744e-06 - accuracy: 1.0000 - val_loss: 1.8460 - val_accuracy: 0.7292\n",
            "Epoch 162/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 1.1519e-05 - accuracy: 1.0000 - val_loss: 1.8279 - val_accuracy: 0.7292\n",
            "Epoch 163/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 6.0560e-06 - accuracy: 1.0000 - val_loss: 1.8297 - val_accuracy: 0.7292\n",
            "Epoch 164/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 6.9161e-06 - accuracy: 1.0000 - val_loss: 1.8329 - val_accuracy: 0.7396\n",
            "Epoch 165/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 6.7256e-06 - accuracy: 1.0000 - val_loss: 1.8345 - val_accuracy: 0.7396\n",
            "Epoch 166/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 7.4014e-06 - accuracy: 1.0000 - val_loss: 1.8372 - val_accuracy: 0.7396\n",
            "Epoch 167/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 9.5822e-06 - accuracy: 1.0000 - val_loss: 1.8385 - val_accuracy: 0.7396\n",
            "Epoch 168/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 7.2511e-06 - accuracy: 1.0000 - val_loss: 1.8428 - val_accuracy: 0.7396\n",
            "Epoch 169/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 6.7464e-06 - accuracy: 1.0000 - val_loss: 1.8470 - val_accuracy: 0.7396\n",
            "Epoch 170/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 5.3808e-06 - accuracy: 1.0000 - val_loss: 1.8494 - val_accuracy: 0.7500\n",
            "Epoch 171/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 5.8913e-06 - accuracy: 1.0000 - val_loss: 1.8532 - val_accuracy: 0.7500\n",
            "Epoch 172/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 6.8574e-06 - accuracy: 1.0000 - val_loss: 1.8551 - val_accuracy: 0.7396\n",
            "Epoch 173/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 4.5867e-06 - accuracy: 1.0000 - val_loss: 1.8634 - val_accuracy: 0.7396\n",
            "Epoch 174/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 5.5745e-06 - accuracy: 1.0000 - val_loss: 1.8662 - val_accuracy: 0.7396\n",
            "Epoch 175/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 1.5727e-05 - accuracy: 1.0000 - val_loss: 1.8765 - val_accuracy: 0.7396\n",
            "Epoch 176/250\n",
            "57/57 [==============================] - 10s 184ms/step - loss: 4.6591e-06 - accuracy: 1.0000 - val_loss: 1.8846 - val_accuracy: 0.7396\n",
            "Epoch 177/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 5.2066e-06 - accuracy: 1.0000 - val_loss: 1.8933 - val_accuracy: 0.7396\n",
            "Epoch 178/250\n",
            "57/57 [==============================] - 11s 185ms/step - loss: 4.8463e-06 - accuracy: 1.0000 - val_loss: 1.9007 - val_accuracy: 0.7396\n",
            "Epoch 179/250\n",
            "57/57 [==============================] - 11s 187ms/step - loss: 3.9403e-06 - accuracy: 1.0000 - val_loss: 1.9081 - val_accuracy: 0.7396\n",
            "Epoch 180/250\n",
            "57/57 [==============================] - 11s 186ms/step - loss: 3.7526e-06 - accuracy: 1.0000 - val_loss: 1.9094 - val_accuracy: 0.7396\n",
            "Epoch 181/250\n",
            "36/57 [=================>............] - ETA: 3s - loss: 3.5444e-06 - accuracy: 1.0000"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have already trained your model and have a `history` object\n",
        "history = multi_cnn_model_CV.fit(x_train_CV, y_train_CV, epochs=250, batch_size=32, validation_data=(x_valid_CV, y_valid_CV))\n",
        "\n",
        "# Plot training vs validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_CV = np.reshape(x_test_CV, (960, 237, 150, 1))"
      ],
      "metadata": {
        "id": "xLvslbMT19-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_CV = multi_cnn_model_CV.predict(x_test_CV)\n",
        "predicted_classes_CV = np.argmax(predictions_CV, axis=1)"
      ],
      "metadata": {
        "id": "R8JhAGu-l69v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_classes_CV = np.argmax(y_test_CV, axis=1)"
      ],
      "metadata": {
        "id": "qBN0X_jhl78x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_CV = np.mean(predicted_classes_CV == true_classes_CV)\n",
        "print(\"Accuracy (CV):\", accuracy_CV)"
      ],
      "metadata": {
        "id": "9Gj9PSQgmAGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted Classes (CV):\", predicted_classes_CV[:10])\n",
        "print(\"True Classes (CV):\", true_classes_CV[:10])"
      ],
      "metadata": {
        "id": "2KIlejUfmG5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}